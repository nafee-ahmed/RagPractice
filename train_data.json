[
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "Is it possible to generate QA pairs without human intervention?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "What's the process called when text is extracted from source files?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "Can you explain what happens during post-processing in this system?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "How does the chunk size affect model performance and retrieval accuracy?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "What's the overall strategy behind using an incremental learning approach like ours?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "How can LLMs be used to enhance customer experience in telecommunications?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "What are some common challenges people face when fine-tuning models for telecom-related queries?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "What's unique about the question-answering approach used by this AI assistant?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "What kind of improvements did researchers notice with incremental fine-tuning compared to the baseline?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Are there any limitations to using this pipeline for telecom queries?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "I'm curious, how do you ensure each answer follows a certain format?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "What's this thing called Ma-tryoshkaLoss and how does it work?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "What happens when you skip tables and images during data chunking?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Did they try any other combinations before finding what worked?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "What's a table that shows some experiment settings and results?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "How do I optimize a large language model for telecom-related tasks?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "What's the main disadvantage of vector search in this context?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "How can I optimize my model's performance given limited computational resources?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "What's the benefit of precise word matching in specialized sectors?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "How do they make sure the full-size embeddings are actually effective?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "Are there any situations where the AI model's output is just not good enough?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "How do I use embedding-based methods for efficient document loading?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "How does using a manual feedback loop impact the overall model performance?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "How does the inference pipeline handle MCQ-like patterns and categories?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "How many times did your session time out on Google Colab Pro?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "How does fine-tuning affect the interpretability of a model, if at all?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "Can you explain how combining loss values from different dimensions affects model performance?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "What happens when the token limit is exceeded in model output?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "What's the name of the new benchmark dataset for large language models in telecommunications?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "Are there any future plans for incorporating multi-modal RAG pipelines?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "What are some common pitfalls to avoid when fine-tuning a model for MCQs?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "Can you tell me about the benefits of fine-tuning on shared documents?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "How do you separate documents into groups in your pipeline?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "How did they make sure all important sections of text were included in the model?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Are there any pre-trained models available for telecom-specific use cases?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "Can you give me an example of how fine-tuning models improves accuracy in specialized domains?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "Can you elaborate on the benefits of instruction fine-tuning for telecom tasks?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "Are there any general best practices for making computation-heavy projects run smoother?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "Can I get more details about your experiences with Compute Canada servers?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "How can you tell when information retrieval is 'time-inefficient' in competition scenarios?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "How does combining vector-based search with keyword-based search improve overall performance on leaderboards?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "How do they handle model responses that don't fit the format?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "What were the top-performing approaches and how well did they do on both leaderboards?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "What are some best practices for deploying LLMs in resource-constrained telecom systems?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "What's the core of the inference process described here?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "How can I boost my model's performance on private data?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "How does the chunk size impact the computational resources required for model training?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Can you walk me through how they handled ambiguous or unclear user queries?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "What's the process like for improving performance of question-answer models?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "What's the optimal way to combine different search methods?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "Can you break down what's being done to enhance general-purpose LLMs for telecom tasks?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "What's the most efficient way to retrieve doc embeddings?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "What's the best way to fine-tune models on large corpora?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "What are some common pitfalls to avoid when fine-tuning a pre-trained model?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "I have a huge dataset, how can I fine-tune it effectively?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "How does the RAG pipeline improve upon previous generative models in terms of accuracy?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "How did MCQ option restrictions affect model output quality?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "What's the outcome when increasing document retrieval numbers?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "How do I optimize an LLM for answering technical questions about 3GPP standards?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "How do you measure the impact of a specialized large language model on business outcomes in telecommunications?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "How does the Matryoshka Representation Learning (MRL) technique work?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Is there a summary of the evaluation results somewhere in this paper?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "What was used instead of traditional training methods due to resource limitations?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "How was the TeleQnA dataset created if I'm interested in learning more?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "Can you give an example of where hybrid search might fail to provide output?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "What's the main issue with fine-tuning models when there's a mismatch between instructions and output?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "What's the best way to fine-tune our model for better results?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "How does your pipeline handle different types of documents?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "How does Phi-2 handle too many documents to process?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "Can you explain incremental learning in simpler terms?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "How do I measure the performance of a specialized large language model in a telecom setting?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "How do I create synthetic datasets for my own QA model?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Do you have any info on applying similar loss functions to other machine learning models?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "How many MCQs were there in total in the TeleQnA dataset?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "How does their model's performance relate to retrieval system quality?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "How does fine-tuning a model improve its ability to retrieve information?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Can you tell me which combination of techniques yielded the best results?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Do I need to convert docx files into something else before using the RAG pipeline?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "Why is it essential to ensure good quality data during model training?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "What's the most recent advancement in document processing pipelines using multi-model architectures?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "Can you walk me through the process of implementing a RAG pipeline for telecom MCQs?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "What makes NDCG useful in their context?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "I'm looking for a benchmark dataset to compare different telecom-LMMs, have you heard about any new ones?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "What are some potential applications of LLMs in network function virtualization (NFV)?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "How does the model handle complexity and nuances in the telecommunication domain?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "Can you give me tips on how to improve the accuracy of an embedding model for telecom-related questions?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "How do you prepare data for fine-tuning a pre-trained model?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "Can you describe how they used the baseline model as a reference point?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "What are some key differences between LLMs and other AI techniques used in telecom?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "How can I fine-tune my model with minimal computing expenses?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "I'm struggling to manage my model's computational demands, help!"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "I'm curious about synthetic QA pairs, can you explain them?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "How does the quality of input data affect document loading and segmentation?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "Can you tell me about methods used to improve the embedding model?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "What kind of comparison was done between different approaches?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "Can you explain how truncated embeddings help with initial retrieval phases?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "Can you explain why choosing the right chunk size is so important in these experiments?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Are there any techniques that can help reduce computing expenses when fine-tuning models?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "How does the model's performance change with different embedding dimensions?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "Can you give me some advice on handling domain-specific jargon and nuances when training an ML model?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "How does the TeleQnA dataset split affect the fine-tuning process?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Is there a specific tool for loading and splitting big documents?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "What's the relationship between synthetic QA generation and embedding model accuracy?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "What's the best way to use RAG pipelines with Phi-2 for generating answers to technical questions?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "What are some examples of using LLMs in telecom standards?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "What methods are being explored to improve the reliability of large language models?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "How do I improve the efficiency of configuration and troubleshooting in telecom systems?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "How do I segment a huge text file for NLP analysis?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "Can you give me some tips on how to fine-tune LLMs for understanding specific domains?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Can you recommend a recent conference on applied artificial intelligence and computing?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "What are some key takeaways from surveys of LLMs in telecom settings?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "What are some potential pitfalls or limitations to keep in mind when fine-tuning models?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "How does the use of advanced document retrieval affect the fine-tuning process?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "How does the choice of chunk size relate to the specific application domain?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "How does having bad-quality data impact the overall performance of models?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "What are the advantages of using vector-based and BM25 search together?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "What happens when you fine-tune a model without a suitable amount of context?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "What adjustments did we make to get a 67% accuracy on the private leaderboard?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Can you describe an efficient approach for fine-tuning models on large datasets?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "What's the best way to fine-tune an existing model on new data without losing its original capabilities?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "What kind of documents are included in the TeleQnA dataset?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Are there plans to open-source or share the code behind this impressive system?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "What kind of telecom-related questions can their pipeline answer easily?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "Can you break down how each component of an RAG pipeline contributes to overall performance?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "What's the deal with gradient checkpointing, how does it help?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "Was there a comparison made between fine-tuned and pre-trained models?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "What's involved in cleaning up incorrect responses?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "Are there any specific industries where RAG pipelines might be particularly useful?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "How does the RAG pipeline help address common issues or limitations of large language models?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "How is text extracted from .docx files?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "Can you tell me about some common mistakes to avoid when finetuning models with different chunk sizes?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "What's the best way to access relevant information in large and complex documents?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "How many documents were provided in the 3GPP dataset?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "How do LLMs contribute to the development of new telecom technologies?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "Can you explain the concept of Retrieval-Augmented Generation in LLMs?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "How accurate was the baseline approach compared to other methods?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "Can you provide examples of how RAG pipelines can outperform baselines by a significant margin?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "Can you tell me about any recent research on telecom-specific LMMs and their evaluation"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "Are there any tips for designing effective prompts for language models?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Can you summarize the first step of setting up the RAG pipeline?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Can you tell me about the role of Matryoshka technique in optimizing data representation?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Does this Ma-tryoshkaLoss thing actually improve model performance?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "Are there any best practices for segmenting 3GPP documents?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Why did you choose 10,000 as the number of synthetic QA pairs?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Is there an optimal chunk size that balances context retrieval with model capacity?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "What's the impact of using pre-trained models on model finetuning and exhaustion?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "How did Harris et al. improve their model's performance?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "How do you ensure that important sections of text aren't lost during tokenization?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "Did using Compute Canada server really make a difference in performance?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "Can you tell me about the main contributions of this proposed RAG pipeline?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "What happens if the AI output doesn't match the expected answer format?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "How did restricted MCQ options cause issues for the model?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Are there any specific frameworks for developing telecom-specific LLMs?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "Are there any methods to prevent or reduce model exhaustion during fine-tuning?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Can you give examples of NLP tasks where the RAG pipeline can be applied?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "What's the relationship between token limit and model performance?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "What fine-tuning methods were used to improve efficiency and accuracy in telecom systems?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "What's the difference between Phi-2 and Falcon in terms of parameters and usage?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Can you tell me about using vector stores for inference processes?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "I'm new to model fine-tuning, where should I start?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "What's the key takeaway from studies on fine-tuning models for improved performance?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "What happens if you exceed the token limit for a Phi-2 model?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "Is it possible to use both vector and keyword-based search mechanisms simultaneously?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "How does the study compare the RAG pipeline's performance to other NLP methods?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "What's the optimal training time for this specific model?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "How do I optimize model performance while avoiding exhaustion?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "Why does increasing chunk size affect answer generation during testing?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "How does a study on LLM improvement techniques apply to real-world use cases?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Is there a specific way they optimized chunk size for this project?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What kind of cleaning is done to model outputs in this pipeline?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "How big were the chunks used in this experiment?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "What's new with language models for telecom networks?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "Is there a specific dataset I should use to train my model for telecom-related tasks?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "What kind of improvements can be expected from using a custom-designed RAG pipeline?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "What kind of method is used to spot remaining mistakes?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "What's the purpose of using a custom prompt template with multiple-choice options?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "What happens when instruction fine-tuning doesn't work well in specialized domains?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "How did they assess the quality of their retrieval system?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "How do I evaluate the performance of my fine-tuned model in a telecom context?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "What library is used to handle and retrieve embeddings?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "What parameter-efficient fine-tuning (PEFT) methods exist for telecom models?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "What's the best way to deal with telecom-related questions when it comes to answering systems?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Can you explain how documents are loaded in the 18th release of 3GPP?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "Can you share a summary of key experiments that show the effectiveness of different search techniques?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "I'm looking for information on how different models were fine-tuned - can you point me to that?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Can you explain low-rank adaptation for fine-tuning big models?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "What's better about using both vector search and BM25 in one go?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "What's the current state of research on specializing large language models for telecom use cases?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "I heard about some new methods for improving model performance, what's the latest?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "What kind of features make an RAG system stand out from others?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Can you explain how they made their system effective for telecom questions?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "Can you explain how user instructions influence document segmentation?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "How does the type of document embedding affect the fine-tuning process?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "Can you provide examples of how generating synthetic QA pairs improves model performance?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Can you walk me through document loading in the RAG pipeline steps?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "How do researchers currently enhance the factual accuracy of LLMs?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "What's the best way to combine keyword-based search with vector-based search for context retrieval?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "Can you explain how generating synthetic QA pairs improves SQuAD2 and NQ performance?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Can you describe document loading as part of the RAG setup?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "What is low-rank adaptation and how does it relate to PEFT in telecom?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "How much of the data had issues that required manual intervention?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "How does the RAG pipeline address the hallucination problem?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "What's the significance of testing retrieval systems at different embedding levels?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Was it a good idea to save the generated QA pairs in a CSV file?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Were there any differences between the public and private leaderboards?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Is there a way to prevent language models from producing false information?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "How do researchers usually evaluate the performance of fine-tuned models?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "What database is used to store the embeddings generated from fine-tuned models?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "What's the ideal size for text segments to process quickly?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Were there any specific criteria used to determine which content to remove?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Can you walk me through the experiment setup for this model?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "Can I skip converting text, tables, and images into images for my telecom model?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "What's the best approach for model finetuning when dealing with limitations of exhaustion?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "Can you tell me more about the human validation process for TeleQnA questions?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "What if I have multiple GPUs to work with, how does that change things?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "How do you generate QA pairs for a large dataset?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Can you explain the importance of alignment datasets for training telecom-specific LLMs?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "Can you show me an example of how to optimize chunk sizes for better results?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "How does document segmentation impact text retrieval efficiency?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "What's the study using to enhance the Phi-2 model's accuracy?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "What were the different configuration settings used for the experiments?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "Can you talk about any lessons learned from implementing this manual feedback loop?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "What's this 'sliding window technique' everyone keeps talking about?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "Can you describe the differences between approaches that use direct answers vs. manual feedback for improving model accuracy?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "Can you modify only a subset of parameters in a model and still get good performance?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "What are some common mistakes people make when fine-tuning an embedding model for a domain-specific task, such as telecom?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "How do they make sure answers adhere to specific format requirements?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "What are some real-world examples of using trained models in telecom applications?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "How can we balance semantic and lexical matching for better information retrieval?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Can you provide more details about the Phi-2 model and its limitations?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "Is there a specific library that's integrated with this vector store for handling and retrieving embeddings?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "Can LLMs be used to extract relevant information from lengthy documents?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "How does answer consistency checking work in generating synthetic QA pairs?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "What are some effective methods for enhancing model accuracy?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "What's the output format of the generated QA pairs, e.g., CSV or JSON?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "Are there any specific standards or regulations that influence TeleQnA content?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "I heard something about rewriting input texts, what's that all about?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "What are the implications of applying proposed methods in this study to other domains?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "How important is it to have good data for fine-tuning models?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "How many approaches were compared in total?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "Can you explain how tokenization works, especially with large documents?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "What are some common challenges in adapting general-domain models to telecom domain?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Can you explain how they got around the maximum token length problem?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "What are some tips and tricks for getting optimal results from your retrieval system when dealing with telecom-related questions?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Is there any potential for using RAG pipeline with other machine learning models beyond Phi-2?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Can you explain why instruction fine-tuning didn't work out for telecom instructions?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "Can you describe the benefits of using synthetic QA pairs in fine-tuning the embedding model?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Are there any trade-offs in using vector-based search?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "What's the relationship between fine-tuning an embedding model and retrieval results?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "What happens when chunk sizes get too big or too small?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "How do I get my language model to understand telecom terminology?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Are there specific steps involved in loading and segmenting documents?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "Can you explain the concept of utilizing 'Phi-2' or 'Falcon' for answering MCQs in telecom-related tasks?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Can I get info on custom loss functions and how they aggregate values?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "How does model finetuning impact overall performance in a specific task?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Can you explain what 'context' means in this NLP context?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Can you describe a typical workflow for implementing the RAG Pipeline?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "Can you explain how the inference engine retrieves relevant document segments for a query?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "What are some potential drawbacks of using a very large chunk size?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "How did the researchers determine the best chunk size for their experiment?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "Can you explain the process of fine-tuning a pre-trained model?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "How many unanswered questions remain even after manual intervention?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "What open-source library did you use for extracting narrative text, paragraphs, and list items from source files?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "Is there a specific chunk size that works well for all models?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "Are there any specific challenges that an RAG pipeline can help with in NLP?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "How does our incremental learning approach address resource limitations in training environments?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "What's the role of embedding models in natural language processing?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "What are the constraints that vector-based search alone can't overcome?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "How does fine-tuning general-purpose LLMs make them more effective for telecom-specific tasks?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "I'm trying to find a sweet spot in my chunk size, any suggestions?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "How can I utilize fine-tuned models to quickly retrieve relevant documents from a large corpus?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Can I get details about fine-tuning LLM and embedding model separately?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "How does the choice of chunk size impact the quality of generated answers?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "Can you describe the processing step that comes after generated answers?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "Can you explain why some methods are more effective than others in reducing model exhaustion?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "Why does instruction sensitivity matter during model training?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "Can you give me some tips on how to fine-tune a pre-trained language model for telecom tasks?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "What was the performance like for approaches that involved embedding and phi-20?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "Can you walk me through how to fine-tune a language model for complex texts?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "How does NDCG help assess retrieval system quality?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "How many different settings were considered for the experiments?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "How many parameters does Phi-2 have compared to Falcon?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "How do you fine-tune models for better performance in specialized fields like telecom?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "Can you tell me about a recent dataset released for evaluating telecom-specific LLMs?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "What's the significance of 'All of the above' or 'None of the above' in TeleQnA questions?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "How does Ma-tryoshkaLoss help model learn important stuff first?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "What's the main goal when implementing a hybrid search technique?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "What's so special about using LoRA instead of other fine-tuning methods?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "What are the key findings in studies that explore new techniques for improving LLMs?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "How do I identify the causes of model exhaustion and prevent them?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "How do retrieval, augmentation, and generation work together in an RAG pipeline?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "Can you explain how to generate questions based on segmented text?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Is improving LLM accuracy an active area of research?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "Can you suggest any techniques to reduce delays during fine-tuning?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "What are some benefits of using LLMs in telecom settings?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "What's the connection between synthetic QA generation and embedding model fine-tuning?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Can you provide an example of how MatryoshkaLoss works in practice?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Can you explain how to enhance the reliability of large language models?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "I'm experimenting with fine-tuning models, what are some things I should keep in mind?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "What's the deal with chunk sizes and embedding methods?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "Are there any private knowledge-bases that utilize LLMs?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "How do you improve the accuracy of a model using a manual feedback loop?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "I'm trying to optimize my text segmentation process. Can someone recommend a good approach?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "How do I deploy a big telecom model on a resource-constrained system?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "Can you talk about improving embedding performances on datasets?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "How do you spot discrepancies between model output and correct answers?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "What were the results of using MRL for embedding fine-tuning?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "How many different embedding models were used for the experiment?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "Is there a way to prevent significant delays during model fine-tuning on full datasets?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "Can using gradient checkpointing and warmup ratios really speed up training?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "What's the best way to specialize large language models for telecommunications applications?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "What's the main idea behind aggregating loss values across different dimensions?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "What's a good way to tackle vocabulary and lack of context in embeddings?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "I need to fine-tune my model for telecom-related questions, what techniques should I use?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "How does rewriting input texts with LLMs affect the overall embedding performances?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "What's the difference between using 1 epoch and 2 epochs in a model?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "How do I implement a robust multi-model pipeline in my own projects?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "Can you tell me more about the retrieval-based component of an RAG pipeline?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "What's the significance of using 3GPP standard documents in training telecom-related language models?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "Can you give examples of NLP tasks that can benefit from the RAG approach?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "What's the deal with document loading in NLP?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Can you recommend some common practices for selecting a suitable chunk size for models like Phi-2?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "How does this pipeline improve answer correctness?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "How does the number of questions in the TeleQnA dataset impact the fine-tuning process?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "Can you provide more context about the creation of the TeleQnA dataset?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "What's the relationship between synthetic QA generation and the embedding model's vocabulary?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Can you describe the benefits of using domain-specific queries in private knowledge bases?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "How does it handle context similarity and relevant terminology better?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "Can I see a summary of all the compared approaches?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "What are some potential downsides to using this method?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "How does TeleQnA contribute to the advancement of AI-based question answering systems?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "How does the Phi-2 model work, and what are its constraints?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "Is there any quality check in the final phase of this process?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "How does fine-tuning an existing model compare to training a new one from scratch?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Did they compare the fine-tuned model's performance with some baseline score?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "How do they extract answers and perform post-processing for evaluation?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "Are there ways to address vocabulary and context issues in language models?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "Can you tell me more about pre-trained phi-2 model as a baseline?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "How do I know if my language model is performing well on complex texts?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Are there any notable applications of LLMs in the telecom industry?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Can I fine-tune a pre-trained model on my own dataset in chunks?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "How can I specialize large language models for telecom use cases?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Is there a link between enhancing LLM accuracy and improving overall performance?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "What are some key considerations when designing a prompt for this approach?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "How does using a pre-trained model affect the effectiveness of finetuning?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "How does the process adapt vocabularies related to the telecommunication domain?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "How do you prepare raw data for the next steps after loading documents?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "How did they measure the effectiveness of their fine-tuning results?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Are there any open-source LLM frameworks specifically designed for telecom use cases?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "How can we improve our model's performance using multi-modal pipelines?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Is there a dataset available for training and testing telecom-related language models?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Are there any open-source libraries that support incremental fine-tuning?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Did you experiment with larger or smaller chunk sizes than 100 words?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "How do people fix vocabulary and context limitations in language models?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "What's the main contribution of this paper regarding LLMs for telecom networks?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "Can you explain how the authors improved their 'Phi-2' model using fine-tuning?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "How does this process affect the performance of the embedding model?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "What were the results of the key experiments mentioned in this paper?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "What advice would you give to someone trying to replicate these results in their own work?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Is it expensive to generate QA pairs for a large telecom-specific dataset?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "What are the crucial processes involved in NLP tasks related to documents?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "How does TeleQnA ensure that generated questions are valid and challenging?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "What techniques are used to fine-tune a pre-trained model?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "What's the best method for improving the accuracy of LLMs?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Are the submissions on both leaderboards done differently?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "I'm curious about the potential for using RAG pipelines with complex queries. Is that something being explored?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "What techniques can help reduce the computational cost of fine-tuning large language models?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "What percentage of the test set is used for the public leaderboard?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "Are there any trade-offs between model performance and training speed when dealing with exhaustion?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "How does the proposed methods in this study improve LLM performance for telecom domain solutions?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "What's the best way to fine-tune an embedding model for complex domains?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "How do you handle data scarcity when fine-tuning a large language model for telecom use cases?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Can you explain how they combined multiple embedding dimensions for their model?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Can you walk me through what's shown in Table II - is it a summary of the top-performing approaches?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "Can you recommend some studies on LLM improvement techniques and their applications?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "What are some efficient ways to fine-tune telecom models without using images?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "How can I utilize LLMs to enhance my QA model's capabilities?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "I'm trying to find an efficient way to split up large chunks of text into smaller pieces. Can you suggest something?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "What's the most effective way to boost the accuracy of question-answering models using synthetic data?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "How does the custom fine-tuned Phi-2 model contribute to the RAG pipeline's performance?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Can you tell me about methods that modify only a subset of model parameters?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "What's the main takeaway from this scenario regarding instruction sensitivity?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Are there any trade-offs between using smaller vs larger chunks during training?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "What library is used to parse text elements like paragraphs and list items?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "What happens if you only have a few thousand QA pairs?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "What's the best way to visualize the performance of a trained model on telecom data?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "Can we skip images in the embedding and fine-tuning process for telecom models?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Did they adjust the tokenizer specifically for their model, like a custom fit?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Is there an ideal way to provide instructions for fine-tuning?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "What's the most effective way to get a higher leaderboard accuracy using fine-tuned models?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Are there any specific requirements for data quality when using MCQs?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "I've heard that fine-tuning models can be tricky. What are some best practices to keep in mind?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "What kind of computational setup is ideal for speeding up model fine-tuning on a large dataset?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "How do you make sure that the generated responses are accurate for multiple-choice questions?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "How do you prioritize which questions or answers to review manually?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Are there any technical standards related to telecoms that were used in this project?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "How does using 'Phi-2' or another model help with optimizing LLMs for telecom?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "How does our approach compare to others in terms of handling multimodal data?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "Are there any potential pitfalls or challenges in using this approach?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "What are some common limitations of instruction fine-tuning that I should know about?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "What are some ways to improve text generation using chunked documents?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "Is it better to use direct answers from LLMs or manual feedback loops for accuracy scoring?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "Why did the model struggle to generate proper outputs during instruction fine-tuning?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "What's the benefit of using a pre-trained Phi-2 model for telecom-related question answering?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "What's a better way to do calculations when Google Colab times out?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Are there any benefits to using a parameter-efficient method like Low-Rank Adaptation?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "How can I leverage the TeleQnA dataset for optimizing an LLM?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "How does our model compare to others in terms of performance improvements?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Can you recommend some tools for document loading in NLP?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "Are there any tricks for keeping crucial context while fine-tuning models?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "Can you explain why the model had trouble generating proper outputs due to instruction issues?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "How does quantization reduce precision - is that like losing data or something?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "What areas outside of telecom could benefit from fine-tuned LLMs?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "How does optimizing embeddings across various dimensions affect model performance?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "What's the optimal way to split a dataset for training and testing MCQs?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "What are some common use cases for Large Language Models?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "Can you explain how to fine-tune a model using a RAG system?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "Can you explain why raw data needs to be handled efficiently before further processing?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "How do I segment complex documents with implicit queries?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "How does context retrieval work in combination with vector and keyword-based search?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "How can we deploy big models on resource-constrained telecom systems?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "What were the different embedding dimensions tested?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Can you explain how to improve Phi-2 model performance using retrieval-augmented generation?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Can I use the RAG pipeline to fine-tune other large language models besides Phi-2?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "Is it possible to generate high-quality synthetic data for my embedding model?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "Can I see a table that shows all the experiment settings?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "What if I need to train a model on a really complex dataset, what should I do?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "How many epochs did they run their model for and what was the goal with NDCG score metrics?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "Can we use low-rank adaptation for fine-tuning big telecom models?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "How do you address the issue of domain shift when fine-tuning a large language model for telecom use?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "What's the most efficient way to generate QA pairs from documents?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Can you give me an idea of the kind of accuracy I could expect from incremental FT methods?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "What are some strategies for overcoming limitations in specialized fields like ours?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "What results came from experimenting with different chunk sizes?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "What's the purpose of combining vector store with a generative model here?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "Are there any pipelines that can enhance telecom domain solutions using LLMs?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "How do researchers augment Large Language Models for better performance?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "What happens if we don't use dimensionality reduction methods in AI models?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "What strategy did you use for dealing with the computational demands?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "What's the key to balancing performance and storage efficiency when dealing with complex domain-specific questions?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "What advantages does the Matryoshka embedding approach bring to retrieval systems, and how are they achieved?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Are there any robust multi-model pipelines for processing documents containing text, tables, and images?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "What happens when we truncate embeddings to lower dimensions?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "What's the best way to use retrieval capabilities with generative abilities?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "Do user instructions always lead to accurate document segmentation?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "How can we make the most out of Retrieval-Augmented Generation (RAG) pipelines?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "How can we fix context limitations in language models?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "Can I use this method to improve the performance of an existing model?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "Is there a specific approach to improving model performance in specialized domains like telecom?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Is there a less computationally intensive model like Phi-2 that we can use?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Are there any specific math modelling benchmarks for evaluating telecom-specific LLMs?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "Is there a specific use case where retrieval-augmented generation really shines?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "What are the unique demands of each field that can be met by fine-tuning general-purpose LLMs?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "How do you know if your fine-tuned model is more effective?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "What's the origin of the TeleQnA dataset?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "Can you explain the importance of post-processing generated answers?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "How long does it take to complete each phase of model fine-tuning?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "Can you explain how to optimize large language models for telecom tasks?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "Can you provide more details about the MRL method used in the experiment?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Are there any takeaways from this experiment that can inform future research?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "Can you describe how to integrate seamless embedding with fine-tuning?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "How does fine-tuning affect the performance of language models in telecom?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "What's the conclusion from experiments regarding instruction-based fine-tuning limitations?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "How do we make sure our embedding model can accurately handle telecom-related queries?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "How do you improve answer correctness in this system?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "Can you show me how they set up their experiment configurations?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "What's involved in making sure model answers are correct and formatted right?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "Can you tell me about the importance of matching instructions and output during fine-tuning?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "What are some best practices for fine-tuning an embedding model for telecommunications applications?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "How do I improve performance in model finetuning without overloading the system?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "Was there a baseline result used to compare with the fine-tuned model?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "How do different chunk sizes impact accuracy in extracting answers?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "How can I quickly identify relevant parts of docs with a query?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "Is there a way to make my model adapt to new data without retraining everything?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "How can we improve coverage and reduce irrelevant results with hybrid search?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "What settings did the team experiment with for fine-tuning their language model?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "How do I ensure my model's performance is not sacrificed when using smaller chunks?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "How can large language models be used in telecom tasks?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What's involved in preparing the final dataset for submission?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "Can you give me more information on why RAG pipelines are useful for optimizing LLMs in telecom?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Is there a way to just focus on early dimensions of embedding vectors?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "Can I use a paid cloud platform for speeding up model training?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "Can you compare and contrast different generative models' ability to address hallucination issues?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "How do you improve the performance of language models with limited data?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "Can you explain why utilizing truncated embeddings during initial retrieval phase is beneficial?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Can you recommend any resources or tutorials on using TelecomGPT and other telecom-specific LLMs?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "Can you explain how to use segmented data from previous chunks for generating relevant questions?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "What kind of setup is required to get the most out of this RAG implementation?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "What's the process for refining answers after they come out?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "What are the three main components of an RAG pipeline?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "Is there a specific way to fine-tune an embedding model for better performance?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "How does the Phi-2 model's architecture impact fine-tuning for MCQs?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Can you point me towards the TeleQnA dataset, please?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "Can you compare the effectiveness of using direct LLM answers vs manual feedback loops for accuracy scoring?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "Can combining multiple steps in a pipeline really boost accuracy that much?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "How do you configure a tokenizer to match the Phi-2 model exactly?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "What's the role of GPT-3.5 API in generating synthetic questions for TeleQnA?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "What's the process to improve text generation using chunked documents?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Is ChromaDB a good choice for storing vector embeddings?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "Can you tell me about the benefits of using sophisticated data processing with fine-tuned models?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "What's the relationship between chunk size and model training epochs?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "What are the benefits of using a smaller subset for QA pair generation?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "Can you give me examples of how to use large language models in telecom tasks?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "How do you make sure the embedding model is fine-tuned for telecom-specific questions?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "Can you explain how the difference between approach 4 and 9 affects the final results?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "How does their approach compare to traditional keyword search methods?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "How long does each phase of fine-tuning take with 33% of the corpus?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "Can you explain how to improve the retrieval process with a fine-tuned pre-trained embedding model?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "What's the relevance of TeleQnA to the field of information retrieval or question answering?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "Can you explain how LoRA helps with maintaining or improving performance on specific tasks?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "Can you give an example of a question generated by GPT-3.5 API for TeleQnA?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "What's the most efficient way to use LLMs for telecom-specific tasks?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "Can you tell me about some common techniques for adapting models to dataset patterns?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Can you explain why we chose Phi-2 over other models?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "Is there a rule of thumb for selecting the perfect chunk size?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "What was the reasoning behind segmenting the data for QA pair generation?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "How do you manage computational resources for large-scale machine learning tasks?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "What's the optimal chunk size for data segmentation?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "What are some advantages of using a combined model like RAG in NLP?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "What kind of instructions are used to guide document segmentation?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "Do synthetic datasets outperform traditional training methods for QA models?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "How did you use regular expressions to extract metadata from your 3GPP release numbers?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Can I use this approach with models other than Phi-2?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "I'd like to know more about fixing vocabulary issues in embeddings."
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "How does this dataset look compared to the original input from the fine-tuned model?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Have you ever used stepwise fine-tuning before?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "How do I get started with using RAG pipelines for optimizing LLMs for telecom-specific tasks?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Can you tell me about the streamlining process that resulted from cleaning up the responses?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "What are some best practices for deploying and integrating specialized large language models in telecom networks?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "I'm trying to improve my model's performance, what's the best way to apply fine-tuning techniques?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "Can you explain how the generated answers are processed next?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "How did the vector store and Phi-2 work together in harmony?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "How do you break down big documents into smaller chunks?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Can you tell me about a model with only 2 billion parameters?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "What kind of text elements are extracted from source files?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "What role do embeddings play in improving performance of LLMs for telecom?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "How does this new technique compare to traditional model training methods?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "What's the role of the unsupervised fine-tuned Phi-2 model in this implementation?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "What's the current state-of-the-art in LLM development?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "Are there any plans to explore other specialized areas like cybersecurity or healthcare for RAG pipeline applications?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "Can synthetic datasets be used to enhance QA model results?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "Is the inference time really that much longer compared to traditional vector search?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Why is it hard for models to work with telecom instructions using instruction-based fine-tuning?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "What makes a 'crucial context' for fine-tuned models and how can I preserve it?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "How do you make your models run faster without upgrading hardware?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Is there a way to reduce the precision of model parameters and still get good results?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "Can you explain how the sliding window technique works in tokenization?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "What's the benefit of only including tables at the fine-tuning phase?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "Can you explain how the 'Phi-2' model was originally trained and how it was improved through fine-tuning?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "Is there a particular reason why 100-token chunks became the standard choice?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "What are some popular methods for document segmentation in NLP?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "What's the best approach to rewriting input texts for better embedding performances?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "Can you tell me about the impact of poor-quality data on model performance?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Is there any information about using 3GPP standards in research publications?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "What chunk size is ideal for fine-tuning models without overloading their token processing capabilities?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Can you tell me about different embedding dimensions & their significance?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "How do researchers handle document loading and segmentation in their work?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "Can you describe how to create a custom embedding model?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Can you describe the chunk size variations in their experiments?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "Are there specific challenges when implementing LLMs in telecom?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "How can I ensure my LLM is properly trained to answer technical questions about 3GPP standards?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "What was so special about combining vector stores and Phi-2 models?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "What are some key takeaways from this study that I can apply to my own work?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "Can you share some strategies for making your model more efficient in computing expenses?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "How do I get started with building my own LLM from scratch?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Can you clarify why Phi-2 token limit was exceeded during document retrieval?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "We need help figuring out how to extract relevant metadata from our telecommunications documents. Any advice?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "We're struggling to find a good balance between text segment size and processing efficiency. Any tips?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Is there a way to make language models more efficient for specific industries?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "Can you walk me through the steps to achieve optimal performance in a retrieval system for telecom-related queries?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "Can you tell me more about post-processing steps for generated answers?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "How do I balance between training speed and model performance when dealing with exhaustion?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "How do I fine-tune large language models for telecom tasks?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "How does the number of documents retrieved impact model output quality?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "What are some common techniques used for model fine-tuning?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "What's the importance of using domain-specific knowledge with LLMs for telecom queries?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "Is there a specific dataset I should use to train an LLM for answering MCQs in telecom?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Is there a community competition focused on specializing large language models for telecom networks?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "Can you recommend a language model that's less resource-intensive for telecom-related queries?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "What's the best way to extract relevant metadata from telecommunications documents like yours?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Can you give me an example of a question that had a response but was missing info?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "I'm curious to know, what's the final loss calculated from combining individual dimensions?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Can you provide some real-world examples of LLMs being used in telecommunications?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Why didn't the model perform well in our experiments?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Are documents stored in a database for better performance?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "How do you balance efficiency and performance in large language model fine-tuning?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "What's a practical solution for computational challenges in fine-tuning?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "What's the trade-off between accuracy and efficiency in embedding vectors?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "Can you tell me about your approach to handling big datasets?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Are there any general principles for optimizing search pipelines?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "What steps were taken to refine the answers from the fine-tuned Phi-2 model?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "How can I get started with developing my own telecom-specific language model?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Why do some models struggle when given large chunks of tokens?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Do they load and segment raw documents from a specific dataset?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "How does the inference time compare to other search methods?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Are there still some questions where the model's response is incomplete?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Can you show me how well different approaches performed on the public leaderboard?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "What's the difference between Phi-2 and Falcon in terms of resource usage?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "Why was it necessary to assess the model's performance at different embedding dimensions?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "How does the Phi-2 model get fine-tuned in this context?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "Can you tell me more about the NDCG score metrics used for evaluation in this context?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Did the cleanup process affect the overall structure of the data?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "What if I have a model that's taking too long to train, what can I try?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "What happens if I train a model too much, leading to exhaustion?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "Can you tell me about the technique that helped us manage computational constraints?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "Can you tell me about optimizing LLMs for use in the telecomm sector?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "What's the benefit of fine-tuning large language models (LLMs) for understanding complex texts?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Is incremental fine-tuning a common technique in deep learning?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "What's the main advantage of using LoRA for fine-tuning large language models?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "How did you integrate document search and embedding in one tool?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "Can I use fine-tuning on any pre-trained model, or are there certain conditions that need to be met?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "I'm having trouble with my model performing well on unseen data. Does incremental fine-tuning help with that?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "How does the RAG pipeline provide context to the generative model?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "Can you tell me about the latest arXiv papers on LLMs?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "What fine-tuning methods are similar to Josi et al.'s approach for multimodal data?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "Can you explain what happens when you retrieve more than one document at a time with Phi-2?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Can you share some resources related to the TeleQnA dataset and 3GPP standards?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Can you give me some examples of how varying chunk sizes affects model performance?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Why did the researchers choose to experiment with different chunk sizes in this study?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "Is there an optimal method for loading and segmenting documents?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Can you recommend any courses, tutorials, or workshops on LLMs for telecom professionals?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Can you point me to where in this paper it talks about our best-performing approach - what was that?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "How does your manual feedback loop work to fix errors in AI output?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Is the dataset used for this research publicly available?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "Can you tell me more about the Unstructured library and its capabilities?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Does increasing the complexity of a search pipeline slow it down?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "How do they optimize performance and ensure correctness during post-processing?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "How can I prepare data for fine-tuning a retrieval system model?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "What's the general procedure for handling complex queries in document processing?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Can you tell me about any issues with fine-tuning the model?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "Can you recommend any popular libraries or tools for training and testing telecom models?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "Can you walk me through each of the six sub-tasks in the RAG pipeline?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "What are some key insights from experiments that compared different approaches to context retrieval?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Are there any alternatives to Phi-2 for vector-based search?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Why save each QA pair in a separate row in the CSV file?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "What problems arise when model instructions and output don't match?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "How are the segmented documents processed in the initial step of the pipeline?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "In what kind of specialized sectors would this hybrid approach be especially useful?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "What's the name of that recent study on evaluating LLMs specifically designed for telecom applications?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "How do you break down large texts into smaller chunks for efficient processing?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "What's the best way to tokenize text without losing important info?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "Can you give me examples of non-essential info removed from answers?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "Can you give me some examples of how new techniques can be applied to improve LLM performance?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Does the study use a new model for answering MCQs or just enhance an existing one?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "What role does vector database play in inference processes?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "What's the process for separating documents into groups during testing phase?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "Can you explain how document retrieval and embedding integration works in a RAG pipeline?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "What happens when you use unsupervised learning for embedding?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "How do advanced fine-tuning techniques differ from standard training methods?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "What are some studies or references that explore new techniques for improving LLM performance?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "I'm trying to fine-tune my model, can you share your tips?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "How do advanced fine-tuning techniques improve model performance?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "What are some common pitfalls to avoid when fine-tuning an embedding model for telecommunications-related questions?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "Can you explain how the augmentation component of an RAG pipeline works?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "I heard there's a new way to assess LLMs in telecom, what's it called?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "I'm trying to improve my model's performance on complex questions \u2013 what techniques would you recommend?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "Are there any open-source LLMs worth checking out?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "What was the purpose of the 554 supporting documents provided for this project?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "How does model performance relate to instruction-based fine-tuning in highly specialized domains?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "Can you explain how to prepare answers for final submission?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Can you explain the concept of specializing large language models and its applications?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "How does document segmentation help improve LLM performance in this context?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Is there a 'sweet spot' chunk size that achieves optimal results in most cases?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "What kind of loss function applies to full-size & truncated embeddings in this setup?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "What are the benefits of using a hybrid search approach over traditional methods?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "How do I determine which methods are effective for reducing model exhaustion in my specific use case?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "How can I use pre-trained models like TelecomGPT as a starting point for my own LLM project?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "What's the role of fine-tuning in generating accurate answers for this model?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "I'm having issues with fine-tuning on full dataset, what's going on?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "What kind of techniques are used to enhance model performance for specific topics?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "Can researchers use language models to help with embedding tasks?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Are there any specific datasets recommended for training telecom-related language models?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "What's the most efficient way to identify relevant parts of documents for a query?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Is this related to document processing or something else entirely?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "What's the best way to compare different approaches for handling model exhaustion?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "How big should a chunk be to prevent model limits from getting exhausted?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "How does research on document loading and segmentation impact practical applications?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "What was the dataset used for this project?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "What's the difference between using LLM-generated answers vs. manual feedback for improving model accuracy?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "How can we improve the accuracy of configuration and troubleshooting in telecom systems?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "How many tokens should I use when finetuning my model for optimal results?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Is image processing involved in their pipeline, or do they avoid it?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Can you explain how the retrieval and generation-based models work together in an NLP task?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "Can you walk me through the process of enhancing an embedding model's capabilities for telecom-related questions?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "What techniques work well for handling exhaustion during model training?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "How can I determine if fine-tuning an existing model is worth the effort for my specific use case?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "How does the chunk size affect the performance of a finetuned model?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "How does one address hallucinations in language models effectively?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "How did you process and organize a huge number of telecommunications documents?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "Can you provide examples of how to use a pre-trained Phi-2 model for telecom-related question answering?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "What were some notable differences between approaches like ins. FT and FT embedding?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "Can you tell me about the advantages of incremental fine-tuning over retraining a model from scratch?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "Can you describe how to implement a RAG pipeline for MCQ answering?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "How does fine-tuning affect the overall performance and complexity of a model?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "Can you describe the process of combining vector-based and BM25 retrieval approaches?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "How can you increase the accuracy of a model in telecommunications?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "Can you explain how incremental fine-tuning works in this context?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "What happens when there's a mismatch between instructions and output?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "How can I improve performance of large language models for answering technical questions?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "How does the study's approach help improve overall performance?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "Did using gradient checkpointing and warmup ratios have any other benefits besides stabilizing training?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "What are some common challenges when developing language models for telecom networks?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "What makes this loss function unique and useful for embeddings?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "Is finetuning enough to get good results when using LLMs?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "What are some effective ways to combine multiple steps in a pipeline for better accuracy?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "How does the TeleQnA dataset handle domain-specific acronyms?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "How do you ensure that the generated answers are contextually relevant within the RAG pipeline?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "What's the main limitation that leads to no outputs in most cases when increasing document retrieval?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "What strategy worked for you guys when dealing with complex models and large datasets?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "What's the point of skipping tables and images in data chunking?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "What's the best way to utilize pre-trained models for improved results?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "How can I improve the efficiency and accuracy of configuration and troubleshooting in telecom systems?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "Why did we decide to skip image processing altogether?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What's the purpose of using regular expressions for cleaning answers?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "What would happen if we only had one epoch of training data?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "I've heard of base models being fine-tuned, how does it work?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Can you recommend a technique for reducing inaccuracies in large language models?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Why wouldn't increasing context always lead to better answer extraction results?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Were there any specific strategies used for fine-tuning these models?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Is there a specific use case where Matryoshka outperforms other embedding methods?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "How does an RAG pipeline provide context for the generative model?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "How does the proposed approach address limitations in instruction fine-tuning, and what are the benefits?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "Is there a specific method for dealing with complex domain-specific questions in retrieval systems?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "What's the main advantage of using Phi-2 over other LLMs for telecom-related tasks?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "Is there a recent study on evaluating LLMs specifically designed for telecom applications?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "Can you share some advice on running multiple epochs effectively?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "I'd love to know more about how answers are assigned numbers for submission."
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "How did the performance change with different embedding models?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "Can large language models be adapted for use in telecommunication networks?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "What strategies can I use to improve answer generation for MCQ questions?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "Can you tell me about how researchers used LLMs for rewriting texts?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "Can you walk me through a case study of improving model accuracy using RAG pipelines?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Are there any notable techniques used in these experiments that I should know about?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "I'd love to hear more about potential future work on including diverse document formats in a multi-modal pipeline."
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "How do I ensure high efficiency in document retrieval?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "How does an RAG pipeline address known issues in generative models?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "Why does instruction sensitivity matter when training models?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "How do you approach data chunking and document loading?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "How do I know if my documents are being efficiently retrieved?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What's the goal of post-processing generated answers?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "Are there any recent studies or papers on using ML for telecom-specific tasks?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Did they use any special tricks to make the model more efficient with fewer parameters?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Can you provide more context about why 'Matryoshka dolls' are relevant in AI?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "I'm looking for a way to incrementally improve my model's performance on new data. Is fine-tuning the solution?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "What are some common issues that arise during data preparation for fine-tuning?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "What's the ideal chunk size when dealing with models and tokens?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "What chunk size did they use to improve model performance?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "What kind of models can benefit from being used in this RAG implementation?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "Can you talk about how you maintain high accuracy in the final dataset?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "What are some key differences between Phi-2 and other AI models?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "Can you provide more details on how context retrieval works with hybrid search methods?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "What's the goal of fine-tuning an embedding model with synthetically generated QA pairs?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Is there a best practice for determining optimal fine-tuning epochs?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Can you explain how doc embeddings are processed and stored?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Was there any systematic approach to handling these kinds of issues?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "What's the key to making a highly efficient info retrieval system like RAG?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Are there any limitations to using MCQs for fine-tuning?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "What's the limitation of instruction-based fine-tuning in specialized domains?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Can you tell me more about splitting the dataset into thirds for fine-tuning?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "I'm trying to understand the optimal results from fine-tuning a model, what's the key?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "How can I use Matryoshka embeddings to improve efficiency in my retrieval system?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "What are the most common use cases for the Unstructured library in telecommunications data processing?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "What's the role of document loading in natural language processing?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "Can you tell me about the ITU AI/ML in 5G Challenge and its focus on optimizing LLMs?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "What's the ideal word count per chunk when breaking down long texts?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "How does our method ensure that textual and tabular data have equal weight?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "What's the most significant advantage of using a combined model in NLP tasks?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Can you walk me through the process of removing unnecessary content?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Can you get around the trade-offs of vector-based search?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "What kind of experiments were performed to achieve best results?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "Can you explain how to enhance model performance using post-processing techniques?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "How can I address domain-specific knowledge gaps when adapting an existing model for telecom use?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "What's the best way to improve performance of LLMs for telecom-specific tasks?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "What's the typical time taken to complete one phase of fine-tuning?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "What happens when you exceed the token limit for a model during training?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "Can you suggest some research papers on LLMs and reasoning?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "What are the implications of using fine-tuning for document loading and segmentation?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "What are some potential applications of this research in real-world telecommunications scenarios?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "How do embeddings get frontloaded with important info using a custom loss function?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "Can you tell me about any potential applications for RAG pipelines beyond just summaries?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "Can you describe some common pitfalls that can lead to model exhaustion during fine-tuning?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "Can you describe the interplay between natural language processing and machine learning in this context?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "What happens when you don't properly clean and prepare data for fine-tuning?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "Can you explain how the pre-trained embedding model is fine-tuned for improved performance?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "What's the main lesson from this scenario regarding fine-tuning model performance?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "How did you ensure that your document chunking process was both efficient and accurate?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Does this technique help models learn info faster or more efficiently?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "How do you make language models more efficient without losing performance?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "How does implementing a multi-modal pipeline enhance model performance?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "Can you give me an example of how tokenization helps during training?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "What's the purpose of fine-tuning the Phi-2 Model?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "What's the goal of manual answer fixing in this system, anyway?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "Can you tell me about a time when incremental fine-tuning saved the day?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "Was a comprehensive evaluation of embedding dimensions conducted?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Can we fine-tune the model using options from the MCQs in training set?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "How does their process ensure that both textual and tabular data get handled correctly?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Can you explain how LLMs can improve telecom network performance?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "How does the AI assistant select the correct answer from multiple-choice options?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "What metric is useful for assessing retrieval systems like theirs?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Are there any challenges associated with implementing LLMs in telecom networks?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "Are there any restrictions on using Phi-2 for answering MCQs in the TeleQnA dataset?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "What's the most time-consuming part of the incremental fine-tuning process?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "Can you recommend any specific techniques for handling exhaustion during model fine-tuning?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "What's the current state-of-the-art in LLM-based telecom research?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Is quantization something new or is it a long-standing technique in AI?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "How do synthetic QA pairs enhance question answering in your system?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "How does this approach compare to other methods for fine-tuning LLMs for telecom tasks?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "Can you describe the process for addressing resource limitations in our training environment?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "How does segmentation help with fine-tuning LLMs for complex documents?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "How do you ensure that each answer follows the expected structure?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "How do fine-tuned models perform compared to their base model counterparts in retrieval tasks?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "I'm trying to find a good chunk size, what was it here?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "What's the role of Large Language Models in natural language processing?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "I'm dealing with complex, domain-specific questions \u2013 what strategies can I use to improve my model's handling of these queries?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "How often do you update your models using new data from the same dataset?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Does the study discuss whether RAG pipeline's benefits apply to more complex tasks?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Can you discuss the trade-offs between fine-tuning and generalization?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "What are some lessons learned from experiments that compared different methods for context retrieval and evaluation?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "Can machine learning improve system performance in this area?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "What did Harris et al. do to boost their model's performance?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "What's the current status of LLM research in 2023 and beyond?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "Can you describe the post-processing loop for generated answers?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "How does the pipeline benefit from efficient raw data handling?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "What kind of improvements do we see in answer correctness here?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Can you explain how Low-Rank Adaptation (LoRA) works and its benefits?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Can this approach be applied more broadly across other research areas?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "What techniques can be used to fine-tune large models without increasing computing costs?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Can you provide some insights on the future of LLMs in the telecom industry?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "How can I improve my language model's performance for telecom-related tasks?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "What's the best way to enhance an embedding model's ability to process telecom-specific queries?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "Can you show me examples of how to increase model performance in niche areas?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Are there any methods for adapting existing models to work with telecom-specific tasks?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "In what kind of domains does hybrid search perform better than traditional methods?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "What are some examples of datasets used for fine-tuning embedding models?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "What happens when we load and segment raw documents?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Are there any advantages to using a larger chunk size than 100 words?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "How do language models benefit from fine-tuning on shared documents?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "What's the main issue with fine-tuning models in this scenario?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "I'd love to know more about how some specific models were combined to achieve better results."
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "What's the final phase of this pipeline and what does it involve?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "I'm interested in learning more about a dataset for assessing telecom-specific large language models"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "What's the training and test setup like for utilizing Phi-2 or Falcon with the TeleQnA dataset?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "Is there any manual feedback loop in this pipeline?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "How do you know if an AI model's output needs manual intervention?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "Can you give me some details about creating a dataset for competition submission?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "How does the combination of loss values from each dimension affect model generalizability?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "What's the connection between RAG pipelines and improved model performance?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "What's the most important factor in making their system effective for telecom questions?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "What kind of context is provided by the retrieval model for the generative one?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "Can you tell me more about how RAG helps find info quickly from big docs?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "Are there any future improvements planned for the performance speed of this new info retrieval method?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "What are some possible applications of large language models in telecom settings?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "How can I improve my model's performance on specific tasks?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "How do you make a machine learning model adapt to telecom industry specifics?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "How can I balance performance and storage efficiency in a retrieval system?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "How did we improve the baseline score by so much?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "Can you explain how synthetic QA pairs are generated with segmented data?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Are there any other ways to improve the performance of the embedding model?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "How did they optimize chunk sizes to achieve better results?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "Can you summarize the best practices for fine-tuning models for telecom-specific MCQ answering?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "How do I avoid delays during model fine-tuning on a large dataset?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "How do you ensure LLM-generated QA pairs are accurate and relevant?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Can you explain why dimensionality reduction is important for AI model performance?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "What are the benefits and drawbacks of using different chunk sizes for finetuning models?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "Are there any other ways to fine-tune embeddings for better results?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "How many configurations were tested in these experiments?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "Can you tell me about LoRA and its benefits in model fine-tuning?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "How does OpenAI's GPT-3.5 API relate to our project?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "What kind of text preprocessing was done on the source documents?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "How do you make sure the model's output matches what's correct?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "What are some ways to make the most out of the resources I have?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "What kind of datasets are available for training models like Phi-2?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Is there a way to adjust the parameters of MatryoshkaLoss for optimal results?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "How do researchers usually identify inaccuracies in large language models?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "How does AI system perform when embeddings are reduced to lower dimensions?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "How do I ensure efficient fine-tuning of large telecom models?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Can you explain the difference between loading and segmenting documents in NLP?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "How does Matryoshka help improve model efficiency without sacrificing accuracy?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "Can you tell me more about how document retrieval and embedding integration happens step by step?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "I'm having trouble with session timeouts, what other options are there?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "What's the most efficient way to fine-tune on 33% of the data?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "What are some effective ways to enhance model accuracy for large language tasks?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Can you explain the role of LLMs in improving network security in telecommunications?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "What happens if a user asks a super-complex telecom question?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "What happens when I implement instruction fine-tuning on the dataset?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "How do you optimize model parameters for efficient processing in telecom?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Are there any active research communities or initiatives focused on applying AI to telecom networks?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "How many multiple-choice questions are included in the dataset used for this research?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "Can you tell me about the post-processing phase of this pipeline?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "What happens when you combine two different search methods?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Can you tell me about segmentation methods used in NLP?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "What are some takeaways from experiments that showed the benefits of combining vector and keyword-based search?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "Are there any immediate plans to apply the proposed methods from this study to other domains?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "How did your team ensure that each document chunk was compact enough for further processing?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "How do you ensure generated answers meet format requirements?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "What's the role of human evaluators in ensuring high-quality output from your AI model?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "Can you share any experience or strategies for dealing with large-scale computations?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "Can you explain how incremental fine-tuning helps with computational constraints?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "Is there a way to improve the embedding model's performance using synthetic data?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "Can you describe an experiment where answers were used directly to get an accuracy score?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "What are some common techniques used to improve the performance of question-answering models?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Is the RAG pipeline used for other AI models or just Phi-2?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Can you tell me about the evaluation setting for these experiments?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "What happens when you use extremely small or large chunks for model training?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "Is there a way to visualize the generated QA pairs for better understanding?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "Are there any limitations to instruction fine-tuning in specialized fields?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "How does using 1 epoch in finetuning affect the overall context retrieval performance?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "I'm trying to fine-tune a model, what are some common steps I should take?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "Can you walk me through your process for optimizing model training time?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "What are some key considerations when fine-tuning a model for MCQ answering?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "How did they try to improve model performance?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "How do different chunking strategies affect processing efficiency?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "Can you explain the core process of generating answers using this pipeline?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "What's the relationship between embedding size and model complexity?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "How does our method avoid overcomplicating the processing of different data types?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "What's the purpose of providing each segment as context for generating questions?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "What's a common way researchers improved embedding model performance on various datasets?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Is there a publicly available implementation of the RAG pipeline for telecom-related tasks?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "How does the Matryoshka embedding approach balance storage efficiency with performance in retrieval tasks?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "How does the use of a publicly available LLM like Phi-2 affect model performance?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "What happens when MCQ options are restricted during fine-tuning?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "How do you divide up your dataset for more efficient training?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "What's the significance of using pre-trained models in this case?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Can you describe the steps involved in implementing the RAG Pipeline?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "What's the process like when generating answers for multiple-choice questions using a RAG pipeline?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "Can you explain how to generate synthetic QA pairs from segmented data for telecom-related applications?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Can you describe what happens when the model initializes with quantization?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "Do you guys deal with any outlier cases where the model gets it wrong?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "Can you explain how to fine-tune models using different training epochs?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "How does LISA compare to other systems for document loading and segmentation?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "How did they fine-tune the LLM and embedding model for this task?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "How can I make my model work faster on big datasets?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "What kind of limitations do models like Phi-2 have when it comes to context size?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Is there a difference in how the RAG pipeline helps for MCQs versus other tasks?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "What's the process like when answering multiple-choice questions with this AI assistant?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "What's the minimum amount of computational resources I need to start with this approach?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Did using MRL technique impact performance at different dimensions?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "Is there a trade-off between model performance and storage space when using fine-tuned embeddings?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "How do you divide 10,000 synthetically generated QA data for training?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Are there new techniques being developed to further improve the factual accuracy of language models?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "What's the main difference between this approach and traditional LLM fine-tuning methods?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "How does TeleQnA dataset differ from other similar datasets?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "Can I use transfer learning to adapt a pre-trained language model for telecom-related tasks?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "How does the retrieval process get enhanced when using this particular setup?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "What's the secret sauce behind handling complex telecom-related queries?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "What are some common challenges when adapting large language models for telecom use?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "What are some real-world applications of the RAG pipeline's retrieval process?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "How did you utilize the telecom-specific MCQs from the dataset?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Can you give me an example of how their system handles a tough query?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "How do you fine-tune large language models for telecom network tasks?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "How did the top 1 matched document retrieval impact model performance?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "Was a pre-trained model used as a reference point?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "What's the advantage of using the RAG approach in generative models?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "What techniques can I use to boost QA model accuracy?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "Can you describe the process of loading documents and separating them into groups?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "Why is precision more important than speed in certain sectors?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Why did the top 1 matched document retrieval strategy help with model efficiency?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "What's the difference when using different chunk sizes on the same model?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "How do you ensure that generated questions are relevant to telecom-specific topics?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "Can you explain how you handle cases when the model gets stuck or doesn't answer questions?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Can chunking strategies be adjusted based on specific use cases?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "How does the generation-based model make use of context provided by retrieval model?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "What's the point of using a manual feedback loop to correct incorrect labels?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "Can you recommend some best practices for fine-tuning models with various document sizes?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Can you explain how document segmentation is used in conjunction with retrieval-augmented generation?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "Can you give an example of what kind of errors your AI model might make?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "Can I combine multiple large language models to improve performance on telecom-related tasks?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Can I use the same method to generate QA pairs for my own dataset?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "How does this technique change the way we understand data representation in AI?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "What's the difference between 'fine-tuning' and 'incremental fine-tuning', if any?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "What's the significance of combining vector store with a generative model in this context?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "What's the relationship between instruction fine-tuning and domain-specific vocabulary?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Can you give me examples of how to apply these concepts practically?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "How can you improve the factual accuracy of language models?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "What are some techniques for enhancing the accuracy of fine-tuned models?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "How do I use LoRA to adapt a pre-trained model to a new domain?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "How did they know which chunk size to use, was it an educated guess or based on previous experience?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "What are some common techniques for fine-tuning models in NLP tasks?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "Are there any specific steps I can take to improve model performance in telecommunications?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "How is text extraction related to document loading and segmentation?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "What's the current state of affairs when it comes to evaluating telecom-LMMs?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "What's the difference between incremental fine-tuning and other types of model adaptation?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "Can you discuss the implications of this study for improving model accuracy in other specialized fields?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "How does quantization affect the precision of model parameters?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "What percentage of answers needed human oversight?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Is it possible to increase accuracy by adjusting the text segment size?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "Is there any specific technique to ensure relevant texts are retrieved in specialized sectors?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "What are some potential applications of large language models in telecommunications networks?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "How does the number of epochs during fine-tuning affect the overall retrieval quality?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "Can you recommend some specialized areas where new methods for improving LLM performance could be applied?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Is it possible to reduce embedding dimensions without affecting model performance?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "Does this improved info retrieval approach impact performance differently under deadline constraints?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "What's the minimum chunk size that can still provide sufficient context for accurate results?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "How many test set percentages were used for evaluation?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "What's the limitation of using instruction-based fine-tuning in telecom domains?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "What was the best-performing approach for improving leaderboard accuracy?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "What's the current state of research on using RAG pipelines with large embedding models?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "What techniques did you use to stabilize the training process?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "What's the RAG pipeline used for in NLP tasks?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Is there a specific data format required for efficient processing?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "What's the sweet spot for token numbers when fine-tuning models?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "How can I speed up computations on Compute Canada servers?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "What's new in the field of LLM reasoning and segmentation?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "I'm not sure how to incorporate incremental fine-tuning into my project, guidance pls!"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "Can you explain what happens when MCQ options are limited during fine-tuning?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "How does the retrieval-based model work in conjunction with generation-based model?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "What are some potential areas where the proposed methods in this study could be applied?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "Can you describe how to generate relevant questions from segmented data?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "How was private leaderboard different from public one?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "What's the role of a manual feedback loop in improving label accuracy?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Can you elaborate on why model performance suffered in telecom instructions?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "How do we select the best model for generating answers to MCQs?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "I'm trying to fine-tune an existing model for telecom use cases. What's the best strategy?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "I'm looking for ways to improve my model's performance, what did they do differently?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "What techniques did they try out to improve accuracy in the competition?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "How does the proposed approach compare to traditional methods for model improvement?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "Does the study mention any specific NLP challenges that the RAG pipeline can solve?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "What are the individual phases of their methodology?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "I'm trying to find a dataset for comparing different telecom-specific language models"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "What are some techniques to enhance QA pair generation?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "What's a common issue with LLMs that this study addresses?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Can I get a rough split of questions in the TeleQnA dataset?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "What's the best way to improve the performance of a pre-trained model?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "What's the role of retrieval-augmented generation in enhancing telecom-specific language models?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "Can you tell me about the processing that happens after model answers are generated?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "Can you provide more context on how our model config reached a 67% accuracy?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "How do you handle cases when an AI model's answers have formatting issues?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "I'm trying to optimize my document processing pipeline. Can you share some best practices?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "Can you describe the process of preparing documents for NLP tasks?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "What was the magic number of chunks used in this study?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "How does the experiment's evaluation phase relate to model performance?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "I want to learn from their experience, what were some key takeaways?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "How many epochs was the model trained for?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "How do I make my language model less prone to errors?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Can you tell me about the process of dimensionality reduction in AI?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Are there any methods for fine-tuning big models in a parameter-efficient way?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "I heard this new method is more precise, can you explain how?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Is there an efficient way to load big documents for NLP tasks?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "Can you tell me what chunk size worked best for this project?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "What benefits do users get from using a model with enhanced RAG pipeline?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "What are some common chunk sizes used for efficient document processing?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "What's this Matryoshka thingy I keep hearing about?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "What's the latest research on applying AI to telecom networks?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "What's a good way to reduce hallucinations in LLMs?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Why not generate QA pairs for the whole dataset from scratch?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "What kind of strengths do you leverage from fine-tuned embeddings in this pipeline?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "How does your system ensure efficient preparation for the next steps in the pipeline?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "How does fine-tuning general-purpose LLMs make them more effective in specialized areas?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "Did they try any new embedding methods?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "What's the retrieval model's role in the RAG pipeline?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "How do LLMs contribute to the development of new telecom protocols?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Can you explain their pipeline for answering telecom questions?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "What's the significance of using fine-tuned models for embeddings?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "Are there any online resources for learning about Large Language Models?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "What's the most significant advantage of using Matryoshka for embedding vectors?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "How does Phi-2's token limit impact output generation?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Is there a limit to how much context you can provide for a Phi-2 model?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "How does model performance affect in highly specialized domains like telecom?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "How does the RAG pipeline address the well-known hallucination problem?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Did they try out different chunk sizes in their experiments?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "Can you suggest some strategies to overcome model limitations in training?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "Were multiple levels of embedding truncation tested?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "What's the approach that combines strengths of both models called?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "What's the point of 'parameter-efficient fine-tuning' anyway?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "How did they combine retrieval and generative abilities in their pipeline?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "Can you give some examples of QA pairs generated using segmented chunks from 3GPP documents?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "How does instruction fine-tuning work in this context?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Is there a straightforward way to load and segment documents for NLP analysis?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "Are there any techniques for creating synthetic QA pairs that I can try?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "Are there any particular telecom-related use cases where RAG pipelines show promise?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "What's the approach they use to answer telecom-related queries?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "What type of documents were used to collect data for TeleQnA?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Are there any specific chunk sizes that are known to work well for models like Phi-2?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "Can you use document chunks to generate more accurate QA pairs?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "I'm experiencing timeouts in my experiments, what are some potential solutions?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "What's the relationship between NLP tasks and document processing?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Is there a study on using RAG (ReasoNer-Augmented-Generator) for LLM improvement?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "I'm looking for surveys on the application of LLMs, got any recommendations?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "What embedding methods did they try out?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "What was the main reason for dividing the main task into six sub-tasks?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "How did they manage computational constraints in their experiment?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Did you encounter any challenges while implementing stepwise fine-tuning?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "Are there any specific techniques to improve text retrieval from chunked documents?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "How do you optimize embeddings across different dimensions?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "What's the best approach for enhancing LLMs for domain-specific telecom use cases?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Is there a way to utilize 3GPP specifications for answering MCQs?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "Can you walk me through the process of fine-tuning a base model for retrieval improvements?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "What are some key benefits of using LLMs in telecommunications?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Can you give me more details on using vector stores in pipelines?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "How does this study's findings relate to real-world applications of search?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Do more complex search pipelines always take longer?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "What's the best way to balance search complexity and speed?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "How does the private leaderboard accuracy compare to public accuracy in general?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "I'm trying to understand how instruction fine-tuning improves general-purpose LLMs. Can someone explain it to me?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "What's the best way to parse and extract text elements from source files like .docx?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Did they do any A/B testing for this project?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "How does finetuning impact the performance of a model when used with hybrid search methods?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "What are some key factors that influenced your decision to use this hybrid retrieval approach?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "What's the point of fine-tuning their model on base model BAAI/bge-base-en-v1?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Can you provide more info on applying robust multi-model RAG pipelines to various documents?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "What are some best practices for implementing a manual feedback loop in model evaluation?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "Can I leverage multi-task learning to adapt a pre-trained language model for telecom-related tasks?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "Is it true that synthetic data can significantly improve model performance on benchmarks like NQ?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "How does Matryoshka compare to other embedding methods?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "What role does tokenization play in the fine-tuning process for telecom-related questions?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "Does hybrid method reduce false positives from semantically related texts?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "How did the incremental fine-tuning of a specific model improve its accuracy?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "How does the model use context clues to improve answer accuracy?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "What's the purpose of nesting smaller data representations within larger ones in AI?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Did using Matryoshka technique lead to better understanding of data representation in AI?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "What was the baseline accuracy I should be aiming for with my model?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Does vector search have some limitations?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Can using multiple search methods really increase inference time?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "Can you compare the results of using 1 epoch vs 2 epochs in model performance?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "I'm looking for information about a benchmark dataset created for testing large language models in telecommunications"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "What are some common pitfalls of relying solely on LLM answers for accuracy scoring?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "Is there a trade-off between precision and coverage when using hybrid search?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Is there a specific limitation to instruction-based fine-tuning in telecom domains?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "How can LLMs be used to optimize telecom network resource allocation?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "How does fine-tuning general-purpose LLMs meet the unique demands of each field?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "What warmup ratios can help with stabilizing the training process?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "How do you use MatryoshkaLoss to frontload essential info into earlier dimensions?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "How is this system's performance improved with each step?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Can I use the same method for other types of data, like text or image classification?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "What's the benefit of using truncated embeddings in an initial retrieval phase?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "Can I use this approach to generate QA pairs for a smaller dataset?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "What are some strategies for improving model performance on telecom-specific MCQs?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "What are some strategies for rectifying incorrect labels generated by LLMs in context retrieval models?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "Can you give me some tips on fine-tuning models with specific chunk sizes?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "What techniques are used to fine-tune an embedding model for telecom-specific use cases?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "What changes would you make if you had to redo the cleaning and formatting for this dataset?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "How did you create 10,000 QA pairs without using the whole dataset?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "How do you determine whether to use 100 or 500 tokens when fine-tuning your model?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Was Low-Rank Adaptation used during fine-tuning of the model?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "How do specialized areas like healthcare or finance benefit from fine-tuning general-purpose LLMs?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "What's the significance of using pre-trained models for telecommunications tasks?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "What steps should I take to handle raw data efficiently?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "Is there a trade-off between accuracy and resource usage when choosing between Phi-2 and Falcon?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What's the final step of this text generation pipeline?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "How does domain-specific vocabulary impact embedding model performance?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "How does combining vector and keyword-based search impact overall performance on public leaderboards?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Can you tell me about any techniques used to handle larger documents during tokenization?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "How do I efficiently fine-tune a model with a massive dataset?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Can you explain why instruction fine-tuning didn't work in this case?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What kind of information is preserved from model responses?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "How do you make a language model more context-aware?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "Can you explain why traditional training methods were insufficient for our needs?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "Is there a way to fine-tune a model without extensive retraining of the entire model?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "How did you make fine-tuning more efficient with incremental approaches?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "Why did we choose to focus on tables rather than images at the fine-tuning phase?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "How do you optimize model performance for telecom applications?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "How does a pipeline like RAG improve telecom domain solutions?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "How does a hybrid search method work with vector and keyword-based search mechanisms?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What happens to the fine-tuned Phi-2 model's responses during this step?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Can you explain what the concept of 'Matryoshka dolls' means in AI?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "What information did they gain from evaluating multiple embedding dimensions?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "How does the question-answering pipeline work with JSON input data?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "What kind of cleaning process do you run on text data before training a model?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "I'm trying to narrow down a large corpus, what techniques can I use for quick and efficient retrieval?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "What's the benefit of emphasizing on optimizing LLMs for telecom-specific tasks through the ITU AI/ML 5G Challenge?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "Why did we need to find alternative methods for enhancing efficiency?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Is custom embedding model fine-tuning part of their pipeline?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "Do you skip tables and images from documents during chunking?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "What percentage of answers might need some human touch to get it right?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Can you explain how the Phi-2 model's responses were rigorously refined?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "Can you walk me through the inference process that's designed for accurate and relevant responses?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Is there a way to make large language models work efficiently without sacrificing performance?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "How do you improve text retrieval from documents?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "What's the overall goal of creating the TeleQnA dataset?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Did the automated cleaning process have any impact on the quality of answers?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "What's the best way to improve accuracy with retrieval-augmented generation?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "Is this system designed to handle complex telecom queries efficiently?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "I'm trying to decide whether to use transfer learning or incremental fine-tuning for my project. Can you help me choose?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "How do I ensure efficient fine-tuning of large telecom models that don't involve image processing?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "How do I make sure my language model is giving accurate answers from complex texts?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Are there specific tools or models used in each phase of their pipeline?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "What's the primary goal of using an incremental learning method like ours?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "How do I know if my fine-tuned model is truly effective in handling complex telecom concepts?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "What techniques did they use to fine-tune their models?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "How do you make sure the dataset stays accurate and reliable?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "Can you explain how proposed methods can enhance telecom domain solutions using LLMs?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Is there any benefit to using the Matryoshka representation learning technique?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Can you explain what phi- is and how it was used?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "What's the main advantage of using both contextual and lexical matching in information retrieval?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "How can we fine-tune an embedding model for telecom-related questions?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "What task is being focused on in the ITU AI/ML in 5G Challenge?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "How can I utilize retrieval-augmented generation for other domain-specific tasks besides telecom?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "How do you fine-tune models for better performance?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "What's the process like when the AI model needs human help to get it right?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "Can I provide more context for better accuracy in MCQ question answering?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "What PEFT methods can be used to improve efficiency and accuracy in telecom systems without using images?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "What are some ways to fine-tune an embedding model for better performance?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "How do you adapt language models to work well in specific domains?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "Can you walk me through a process for improving text generation?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "What kind of GPUs do you need for fine-tuning with gradient checkpointing?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "Can you discuss the trade-offs between relying on AI and human input?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "How did your experiment compare with the competition's evaluation phase results?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "Are there methods that help with embedding tasks on various datasets?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "Is there a way to narrow down relevant documents from a large corpus using truncated embeddings?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "What are the most common challenges when fine-tuning LLMs for understanding complex documents?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "How does post-processing ensure correct and formatted answers?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "What are some common mistakes to avoid when applying manual feedback to improve model accuracy?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "Are there any studies that explore new techniques for improving LLMs?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "We need to break down our large dataset into smaller chunks. Can someone show me a sample workflow?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "Can you discuss the importance of applying AI to telecom networks and its future implications?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "How do I know when it's time to switch from one cloud platform to another for computation?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "How do I optimize document loading for faster NLP processing?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Are there any real-world use cases where this pipeline would shine?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "What's the best way to fine-tune a pre-trained model for QA pairs?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "How does the retrieval-based model help with answering MCQs?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "What happens when you remove HTML tags and extra spaces from text?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "How do they break down their task into smaller steps?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "Is there a more efficient method for narrowing down relevant documents from a large corpus?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "What's the latest research on Large Language Models?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Was accuracy score calculated just based on public leaderboard?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "Is there a way to make the synthetic QA generation process more efficient?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "What's the key takeaway from this study about search limitations?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "How do they refine answers to ensure correctness and format?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "How do they ensure only essential info is preserved from model outputs?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "How does this compare to other research studies on search efficiency?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "What are some tips for enhancing model performance in answering telecom-specific MCQs?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "Can you explain how to design effective prompts for language models?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "Can you explain how document segmentation contributes to improved model accuracy?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Is LoRA a good approach to make language models work well in specific domains?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "What role do synthetic QA pairs play in enhancing the embedding model's accuracy?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "Are there any other models we could use for better results, like Phi-2?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "I'm trying to understand how instruction fine-tuning impacts model performance in specialized areas like telecom."
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "Are there any established benchmarks or evaluation metrics for specialized large language models in telecom?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "Can you compare and contrast this new method with traditional vector search?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "How do I implement an RAG pipeline with Phi-2 to answer technical questions about 3GPP standards?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "What's the problem with using only vector-based retrieval for searching?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "Is there a recommended approach for balancing instruction quality and quantity?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "What steps are involved in fine-tuning a pre-trained embedding model?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "How do you prevent over-retrieval of texts that are semantically related but syntactically irrelevant?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Can you explain how model quantization affects performance and efficiency?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "How do systems like LISA handle complex queries for document segmentation?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Are there any plans to extend or improve upon this research in the future?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "What role do synthetic datasets play in improving the accuracy of QA models?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "What metrics were used to evaluate the model's performance?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "How does our approach avoid complexity when dealing with different types of data?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "What kind of accuracy did we see for the approach that involved incremental FT with HS?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "How does the generative model use context to improve output quality?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "How did you improve efficiency for large training datasets?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "How can we improve the accuracy of language models in telecommunications?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Can you describe a scenario where smaller embeddings are actually better?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "I'm trying to enhance my LLM's factual accuracy, got any tips?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Were different fine-tuning techniques tested separately or together?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "How can I enhance the trustworthiness of my language model?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "How do you prevent resource usage from getting too high?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "How do you apply manual feedback to improve model accuracy on private leaderboards?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "Can you fine-tune an embedding model with synthetic QA data?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "Can you point me to any papers or research related to the development of telecom-specific language models?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "What's the purpose of evaluating the fine-tuned model on a baseline score, and how is it used to assess improvements?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "How do I make sure my retrieval process is highly efficient?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "How does the RAG approach compare to other generative models in terms of hallucination rates?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "How does the model handle ambiguity or uncertainty in input queries?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "What strategies can I use to optimize storage space without compromising retrieval quality using fine-tuned embeddings?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "How does this study contribute to the development of more efficient LLMs?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "How does the prompt template ensure each document chunk is provided as an input?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "Can you tell me about the system for reviewing and fixing errors?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "How does the Phi-2 model's token limit impact text segmentation decisions?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "Are there any examples of successful large language model deployments in telecom networks?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Is there a way to parallelize each phase of the incremental fine-tuning?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "What's the deal with setting token lengths and strides - is that important?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "Are there limitations to using user instructions for document segmentation?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "What's the best way to fine-tune a model for specific tasks like MCQs?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "How does using larger chunk sizes affect the overall model accuracy?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "How did you use regular expressions to clean up your 3GPP release numbers and make them more usable?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "What are the key considerations when choosing a document chunk size for model training?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Can you tell me about zero-shot learning in document loading and segmentation?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Can you tell me about the integration of libraries in this context?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Can you provide some guidelines on selecting an appropriate chunk size for a new project?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "Can you describe how we handle multimodal data in our RAG pipeline?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "What happened when you tried using Google Colab Pro for computation?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "How can I overcome computational constraints while fine-tuning a model?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "How do LLMs counter hallucinations and improve factual accuracy?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "How do I prepare pre-training, instruction, and alignment datasets for a telecom-specific LLM?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "What's the point of using quantization in model training?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "How did they decide to use 100 tokens as the standard size?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "How does TeleQnA utilize domain-specific knowledge to generate questions?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "Can you tell me about the best practice for training on a large dataset?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Was the dataset split into training and test sets?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "How does using two different methods simultaneously affect processing speed?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "Can you explain why Phi-2 is preferred over Falcon for certain telecom tasks?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "How can I fine-tune general-purpose LLMs for specific domains like healthcare or finance?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "Is there anything special about rewriting texts for the model?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "What are the consequences of ignoring model exhaustion during fine-tuning?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Can more context always lead to better answer extraction?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "How was the TeleQnA dataset created?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "Were there any questions left unanswered by the model?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "Can you walk me through how rigorous post-processing helps in info retrieval?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Did you try anything fancy with the submission strategy?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Are there any comparisons or discussions about how different techniques performed?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "How did you manage a huge dataset for fine-tuning?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "Can you walk me through the process of embedding finetuning for improved results?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Can I get more info on the chunk size optimization experiments done here?"
    }
]
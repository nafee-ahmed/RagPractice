{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"llama3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 6\n",
      "Length of a page: 6103\n",
      "Content of a page: II. R ELATED WORKS\n",
      "Document loading and segmentation are two crucial pro-\n",
      "cesses for NLP tasks. Lai et al. introduced a system named\n",
      "LISA which can handle complex, implicit queries by segmen-\n",
      "tation documents based on user instructions. One of the main\n",
      "capabilities of the tool is that it can produce segmentation\n",
      "from embedding directly. This system demonstrates its zero-\n",
      "shot abilities and robust performances even with limited data\n",
      "for fine-tuning. [5]\n",
      "Karapantelakis et al. explored the use of LLM for under-\n",
      "standing telecommunication standards. They fine-tuned LLMs\n",
      "to handle large and complex documents by providing faster\n",
      "access to relevant information. They also demonstrate how pre-\n",
      "processing as well as segmentation can contribute to increasing\n",
      "the accuracy of a fine-tuned model. [6]\n",
      "To improve performance of question-answer (QA) models,\n",
      "Alberti et al. developed a technique to generate synthetic QA\n",
      "pairs. The overall process involves generating questions based\n",
      "on segmented text and validating through answer consistency\n",
      "checks. The authors demonstrate how utilizing these synthetic\n",
      "datasets significantly improves the performance of QA models\n",
      "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\n",
      "Harris et al. also followed a similar approach of generating\n",
      "synthetic QA pairs to improve the performance of the embed-\n",
      "ding model. To address the limitation of vocabulary and lack\n",
      "of context, authors use LLMs to rewrite input texts which\n",
      "showed significant improvement in embedding performances\n",
      "on various datasets for embedding model’s fine tune. [8]\n",
      "Zou et al. proposed TelecomGPT, a telecom-specific LLM\n",
      "framework [10]. Authors gathered and prepared pre-training,\n",
      "instruction, and alignment datasets as well as created Telecom\n",
      "Math Modelling, Telecom Open QnA, and Telecom Code\n",
      "benchmarks for evaluation. TelecomGPT surpassed GPT-4,\n",
      "Llama-3, and Mistral in these benchmarks for 3GPP document\n",
      "categorization, telecom code generation, and math modelling\n",
      "in telecommunications.\n",
      "Zhou et al. surveyed LLMs in telecom and highlighted\n",
      "parameter-efficient fine-tuning (PEFT) methods including low-\n",
      "rank adaptation for fine-tuning big models [9]. The models can\n",
      "be deployed to resource constraint telecom systems to improve\n",
      "efficiency and accuracy of configuration and troubleshooting.\n",
      "Along with PEFT, we needed to follow an incremental learning\n",
      "approach to address resource limits in our training environ-\n",
      "ment.\n",
      "Our RAG pipeline shares similarities with Josi et al.’s one\n",
      "[14], particularly addressing multimodal data. Unlike their\n",
      "method of converting text, tables, and images into images,\n",
      "we chose to skip the images in both the embedding and fine-\n",
      "tuning. We included the tables only at fine-tuning phase. Our\n",
      "technique ensures predominant behavior of both textual and\n",
      "tabular data and avoids complexity of image processing.\n",
      "III. M ETHODOLOGY\n",
      "In this section, we discuss our proposed approaches for\n",
      "answering telecom-specific questions using the RAG pipeline\n",
      "in detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\n",
      "Load and Segmentation, (2) Synthetic QA pair Generation,\n",
      "(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\n",
      "the Phi-2 Model, (5) Implementation of the RAG Pipeline,\n",
      "and (6) Answer extraction & post-processing step for result\n",
      "evaluation.\n",
      "A. Documents Loading and Segmenting\n",
      "In the first step of the RAG pipeline, we load and segment\n",
      "the raw documents from the 3GPP Release 18 dataset. It\n",
      "contains technical standards related to the telecommunications\n",
      "domain, and the 554 documents were provided in .docx format.\n",
      "We segregate them into more manageable chunks to properly\n",
      "fit into the vector database.\n",
      "We used the open-source Unstructured library to extract\n",
      "various text elements, such as narrative text, paragraphs, &\n",
      "list items, from the source files. This library helped us parse\n",
      "the documents and relevant metadata, such as the 3GPP release\n",
      "number, which was extracted using regular expressions. Then\n",
      "the documents were loaded and the text divided into smaller,\n",
      "manageable chunks. Each chunk was 100 words in length, a\n",
      "size chosen to ensure that the text segments were compact\n",
      "enough for efficient processing in subsequent stages of our\n",
      "pipeline. For the document chunking, we appended the text to\n",
      "an existing segment or started a new one, depending on the\n",
      "length of the current segment. We also experimented with a\n",
      "500-token chunk size with the assumption that more context\n",
      "would result in better accuracy in extracting answers for MCQ\n",
      "questions. However, our experiments revealed that the token\n",
      "limit of the Phi-2 model is 2048 tokens. If we provide a\n",
      "larger chunk size for better context, the model fails to generate\n",
      "correct answers during the testing phase. This step for loading\n",
      "documents and separating them into groups made sure that\n",
      "the raw data was handled efficiently and prepared for the next\n",
      "steps in our pipeline. In our data chunking, we skipped the\n",
      "tables and images from the documents.\n",
      "B. Synthetic QA Generation\n",
      "We generate synthetic QA pairs with the segmented data\n",
      "from the previous step of our pipeline. These pairs are crucial\n",
      "for fine-tuning the embedding model and for enhancing its\n",
      "ability to accurately process the telecom-specific questions.\n",
      "Each segment from the previous chunks is provided as the\n",
      "context for generating relevant questions. To generate the\n",
      "QA pairs, we designed a prompt template to ensure that\n",
      "each document chunk is provided as an input and the LLM\n",
      "generates a synthetic question from that document chunk.\n",
      "We used the pre-trained Phi-2 model from the Hugging Face\n",
      "pipeline and LangChain framework for this task. We generated\n",
      "a total of 10,000 synthetic QA pairs from the segmented\n",
      "data, instead of creating QA pairs for the whole dataset. Our\n",
      "intuition is that, in the next step of our pipeline, the embedding\n",
      "model will be well-trained with the vocabulary that exists\n",
      "in these 10,000 data rows as they cover a large number of\n",
      "telecom-specific vocabulary. Also, the synthetic QA generation\n",
      "process is computationally expensive and time-consuming to\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(file_path=\"./pdf2.pdf\")\n",
    "pages = []\n",
    "async for page in loader.alazy_load():\n",
    "    pages.append(page)\n",
    "\n",
    "print(f\"Number of pages: {len(pages)}\")\n",
    "print(f\"Length of a page: {len(pages[1].page_content)}\")\n",
    "print(f\"Content of a page: {pages[1].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 26\n",
      "Length of a chunk: 1493\n",
      "Content of a chunk: I. I NTRODUCTION\n",
      "Large language models’ (LLMs) rapid evolution has rev-\n",
      "olutionized natural language processing (NLP) in numerous\n",
      "domains. However, the use of LLMs in the telecommunica-\n",
      "tions sector has not been extensively implemented, especially\n",
      "in tasks that require specific domain knowledge, such as\n",
      "providing answers to technical questions based on 3GPP\n",
      "standards. Using the TeleQnA [12] dataset, the ITU AI/ML\n",
      "in 5G Challenge brings an opportunity to address this gap by\n",
      "emphasizing on optimizing LLMs for telecom-specific tasks.\n",
      "In this challenge, the task is to utilize either “Phi-2” [1] or\n",
      "“Falcon” [2] to answer the MCQs in the TeleQnA dataset.\n",
      "We design an RAG pipeline that utilizes the “Phi-2” model\n",
      "to generate the answers to the MCQs. The reason behind\n",
      "selecting “Phi-2” is that, it is less resource intensive compared\n",
      "to Falcon. Falcon has seven billion parameters whereas Phi-\n",
      "2 has two billion. The training and test sets are provided on\n",
      "TeleQnA dataset. One restriction on using “Phi-2” is that we\n",
      "cannot fine-tune the model using the options of the MCQs in\n",
      "the training set. A set of 3GPP specifications is shared with\n",
      "us that can be utilized as necessary. These documents contain\n",
      "information that is necessary to answer the MCQs correctly.\n",
      "The TeleQnA dataset is created by collecting documents\n",
      "from 3GPP standards, research publications, and overview\n",
      "[12]. OpenAI’s GPT-3.5 API is utilized to generate synthetic\n",
      "questions from the collected and processed documents. The\n"
     ]
    }
   ],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(pages)\n",
    "\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "print(f\"Length of a chunk: {len(chunks[1].page_content)}\")\n",
    "print(f\"Content of a chunk: {chunks[1].page_content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can use any other embedding model but better to use the one that comes with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 26/26 [00:57<00:00,  2.21s/it]\n"
     ]
    }
   ],
   "source": [
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    collection_name=\"rag-llm\",\n",
    "    embedding=OllamaEmbeddings(model='nomic-embed-text', show_progress=True),\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:02<00:00,  2.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 5, 'source': './pdf2.pdf'}, page_content='semantically related but syntactically irrelevant texts, and\\nprovides precise word matching. It is especially useful in\\nspecialized sectors where contextual similarity and relevant\\nterminology are both critical. The hybrid method addresses the\\nconstraints of vector-based search alone, resulting in a more\\nextensive and accurate retrieval procedure. However, in our\\nexperiments, the inference time was twice as long as that of the\\nvector search. This is because two different methods were used\\nsimultaneously, resulting in a time-inefficient pipeline given\\nthe deadline constraint of the competition.\\nThe baseline results using the pre-trained Phi-2 with the pre-\\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\\nfor our experiments. The significant difference between our\\nbest result and baseline demonstrates the efficiency of our\\npipeline in greatly enhancing the performance of the model.\\nV. C ONCLUSION & F UTURE WORKS\\nThe goal of this study is to improve the Phi-2 model’s\\nperformance in the field of telecommunications. Our best-\\nperforming model configuration reached a 67% accuracy on\\nthe private leaderboard, improving the baseline score by\\n45.20%. Significant improvements in accuracy are achieved\\nby fine-tuning the pre-trained Phi-2 model and using MRL for\\nembedding finetuning. The incremental fine-tuning technique\\nproved efficient in managing the computational constraints,\\nwhich resulted in a feasible solution for this task.'),\n",
       " Document(metadata={'page': 5, 'source': './pdf2.pdf'}, page_content='semantically related but syntactically irrelevant texts, and\\nprovides precise word matching. It is especially useful in\\nspecialized sectors where contextual similarity and relevant\\nterminology are both critical. The hybrid method addresses the\\nconstraints of vector-based search alone, resulting in a more\\nextensive and accurate retrieval procedure. However, in our\\nexperiments, the inference time was twice as long as that of the\\nvector search. This is because two different methods were used\\nsimultaneously, resulting in a time-inefficient pipeline given\\nthe deadline constraint of the competition.\\nThe baseline results using the pre-trained Phi-2 with the pre-\\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\\nfor our experiments. The significant difference between our\\nbest result and baseline demonstrates the efficiency of our\\npipeline in greatly enhancing the performance of the model.\\nV. C ONCLUSION & F UTURE WORKS\\nThe goal of this study is to improve the Phi-2 model’s\\nperformance in the field of telecommunications. Our best-\\nperforming model configuration reached a 67% accuracy on\\nthe private leaderboard, improving the baseline score by\\n45.20%. Significant improvements in accuracy are achieved\\nby fine-tuning the pre-trained Phi-2 model and using MRL for\\nembedding finetuning. The incremental fine-tuning technique\\nproved efficient in managing the computational constraints,\\nwhich resulted in a feasible solution for this task.'),\n",
       " Document(metadata={'page': 5, 'source': './pdf2.pdf'}, page_content='leaderboard accuracy, substantially improving the baseline\\naccuracy of 21.8%. This configuration allowed the model to\\nbetter adapt to the dataset’s pattern. The 100-token chunk\\nsize was ideal for keeping crucial context without exceeding\\nthe model’s token processing capabilities, resulting in better\\nretrieval and generation accuracy. The use of MRL was pivotal\\nin improving model performance. By distributing embedding\\ninformation across multiple dimensions, this approach enabled\\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\\nretrieve relevant context and learn the domain-specific vocab-\\nulary. The instruction fine-tuning did not perform well in our\\nexperiments. The model struggled with telecom-specific in-\\nstructions, leading to poor results. This outcome demonstrates\\na limitation in the application of instruction-based fine-tuning\\nwithin highly specialized domains. In all our experiments,\\ngiven the input question we retrieved the top 1 matched\\ndocument as the context from the vector database. Increasing\\nthe number of documents retrieved led to the exhaustion of\\nPhi-2’s token limit, hence resulting in generating no outputs\\nin most cases.\\nWe also implemented a hybrid search technique that com-\\nbines vector-based and BM25 retrieval approaches to enhance\\ninformation retrieval through semantic and lexical match-\\ning. This improves coverage, decreases the risk of retrieving\\nsemantically related but syntactically irrelevant texts, and'),\n",
       " Document(metadata={'page': 5, 'source': './pdf2.pdf'}, page_content='leaderboard accuracy, substantially improving the baseline\\naccuracy of 21.8%. This configuration allowed the model to\\nbetter adapt to the dataset’s pattern. The 100-token chunk\\nsize was ideal for keeping crucial context without exceeding\\nthe model’s token processing capabilities, resulting in better\\nretrieval and generation accuracy. The use of MRL was pivotal\\nin improving model performance. By distributing embedding\\ninformation across multiple dimensions, this approach enabled\\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\\nretrieve relevant context and learn the domain-specific vocab-\\nulary. The instruction fine-tuning did not perform well in our\\nexperiments. The model struggled with telecom-specific in-\\nstructions, leading to poor results. This outcome demonstrates\\na limitation in the application of instruction-based fine-tuning\\nwithin highly specialized domains. In all our experiments,\\ngiven the input question we retrieved the top 1 matched\\ndocument as the context from the vector database. Increasing\\nthe number of documents retrieved led to the exhaustion of\\nPhi-2’s token limit, hence resulting in generating no outputs\\nin most cases.\\nWe also implemented a hybrid search technique that com-\\nbines vector-based and BM25 retrieval approaches to enhance\\ninformation retrieval through semantic and lexical match-\\ning. This improves coverage, decreases the risk of retrieving\\nsemantically related but syntactically irrelevant texts, and')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever()\n",
    "retriever.invoke(\"Why did the author not use Falcon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The current Prime Minister of Bangladesh is Sheikh Hasina. She has been serving as the Prime Minister since January 2009, and her party, the Awami League, won a landslide victory in the 2014 general election, which she led to power.\\n\\nSheikh Hasina is also the leader of the Awami League and has been a key figure in Bangladesh's politics for over four decades. She served as Prime Minister from 1996-2001 and again from 2009-present.\\n\\nIt's worth noting that Bangladesh has a parliamentary system, where the Prime Minister is the head of government and is responsible for appointing ministers to various portfolios.\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-10-13T20:37:40.1720922Z', 'message': {'role': 'assistant', 'content': ''}, 'done_reason': 'stop', 'done': True, 'total_duration': 29805088700, 'load_duration': 28815400, 'prompt_eval_count': 18, 'prompt_eval_duration': 1127460000, 'eval_count': 132, 'eval_duration': 28646202000}, id='run-2bfbce33-7585-4ae2-8ace-a618a180c34e-0', usage_metadata={'input_tokens': 18, 'output_tokens': 132, 'total_tokens': 150})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOllama(model=MODEL, temperature=0)\n",
    "model.invoke(\"Who is the prime minister of Bangladesh?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current President of Bangladesh is Abdul Hamid. He has been serving as the 20th President of Bangladesh since April 2009, when he took office after the resignation of Iajuddin Ahmed. Prior to becoming President, Abdul Hamid served as Speaker of the Jatiya Sangsad (National Parliament) from 1991 to 1996 and again from 2009 to 2013.\n",
      "\n",
      "However, it's worth noting that Bangladesh is a parliamentary democracy, and the Prime Minister is the head of government, while the President serves as the ceremonial head of state. The current Prime Minister of Bangladesh is Sheikh Hasina, who has been in office since January 2009.\n"
     ]
    }
   ],
   "source": [
    "parser = StrOutputParser() # formats model response to only have message and not instance\n",
    "# input to the parser is the output of the model\n",
    "chain = model | parser\n",
    "print(chain.invoke(\"Who is the president of Bangladesh?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an assistant that provides answers to questions based on a given context.\n",
      "\n",
      "Answer the question based on the context. If you can't answer the question, reply \"I don't know\".\n",
      "\n",
      "Be as concise as possible and go straight to the point.\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is the question\n",
      "\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"\n",
    "You are an assistant that provides answers to questions based on a given context.\n",
    "\n",
    "Answer the question based on the context. If you can't answer the question, reply \"I don't know\".\n",
    "\n",
    "Be as concise as possible and go straight to the point.\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "# we can pass context and question like to template. For testing purposes:\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is the question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For chains in langchain, whatever comes before is the input to what comes after like for a | b | c, b's input is the output of a, and c's input is the output of b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is this about?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:03<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: This document discusses a proposed approach for answering telecom-specific questions using the RAG pipeline and related methodologies.\n",
      "****\n",
      "Question: Why is Falcon not used?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:03<00:00,  3.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know.\n",
      "****\n",
      "Question: Why did the author choose Phi-2 over other models?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OllamaEmbeddings: 100%|██████████| 1/1 [00:03<00:00,  3.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: I don't know.\n",
      "****\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    \"What is this about?\",\n",
    "    \"Why is Falcon not used?\",\n",
    "    \"Why did the author choose Phi-2 over other models?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({\"question\": question})}\")\n",
    "    print(\"****\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

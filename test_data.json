[
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "Can you walk me through the process of generating questions for TeleQnA?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "How does the chosen chunk size impact answer extraction accuracy?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "Is there a recent publication on benchmarking large language models for telecommunication use cases?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "Can I use this approach for other domains besides telecommunication?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "How do you evaluate the effectiveness of a specialized large language model in a telecom context?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "Which configuration settings showed the most improvement?"
    },
    {
        "chunk": "Large Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large\nLanguage Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks",
        "question": "Are there any open-source tools or frameworks for specializing large language models for telecom?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "How does the GPU memory of 40 GB on the Compute Canada server compare to our initial setup?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "How do you make sure the generated responses are accurate and contextually relevant within a RAG setup?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Is there a tool that can help with efficient document loading and segmentation?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "What metric was used to evaluate the baseline model?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "Did you try any other methods before deciding on incremental fine-tuning?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Is there some documentation on embedding dimensions & their use in modeling?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "How does custom fine-tuning and incremental fine-tuning contribute to improved model performance?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Can I use a similar method to create QA pairs for other NLP tasks?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "Is this whole 'Low-Rank Adaptation' thing essential for the Phi-2 model?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "What are the advantages of using this particular approach?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "How do NLP systems load documents in real-time?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "How do I troubleshoot issues with timed out sessions during cloud computation?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "What are the benefits of using pre-processing in fine-tuning models for complex documents?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "How does generating synthetic QA pairs affect model performance?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "What kind of system can segment documents based on user input?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "What's the importance of document loading and segmentation in NLP tasks?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Are docx files compatible with the RAG pipeline out-of-the-box?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "How does combining vector-based and BM25 retrieval approaches improve info retrieval?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "Can you explain how our approach differs from converting text, tables, and images into images?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "How does one submission get evaluated on both leaderboards?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "What's the token limit for the Phi-2 model?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "What's the current state of using large language models in the telecomm sector?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "I'm curious to know if there's a dataset created for testing telecom-specific language models"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "What's the main advantage of using gradient checkpointing during fine-tuning?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "What's the secret to making QA models perform better on benchmarks like SQuAD2 and NQ?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "How were the source documents cleaned before training the model?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "How does this process reduce computational costs?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "What are some common mistakes to avoid when trying to improve model accuracy in niche areas?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "How is the quality of generated questions ensured in TeleQnA?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "Are there any better ways to do multi-method searches?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "How do LLMs interact with other technologies like AI and IoT in telecom?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "What kind of data formatting changes were made during this cleanup process?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "What happens when we parse text from source files?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "How do I break up a long text into smaller chunks that are efficient for processing?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "I'm trying to achieve optimal results from my retrieval system, what techniques should I apply?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "How can I ensure efficient resource usage when training models?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "Why does my model fail to generate answers when I provide a larger chunk size?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "What's TeleQnA and how does it relate to large language models in the field of telecommunications?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "What happens when you combine different chunk sizes?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "What can be learned from this study about model fine-tuning strategies?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "How do LLMs perform compared to other AI models?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "How does combining different techniques help improve model accuracy?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "What's the most effective way to load and segment big data for NLP analysis?"
    },
    {
        "chunk": "tasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,\nsynthetic question-answer (QA) generation, custom fine-tuning\nof the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the",
        "question": "How does the proposed approach compare to other methods for improving model performance?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "Can you tell me what happened when they experimented with smaller chunks?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "What's the role of Phi-2 Model fine-tuning in this process?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "What's the trade-off between retrieval and generation stages in the pipeline?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "What's the reasoning behind using Phi-2 and MRL together in this model config?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "What happens when models fail to generate correct answers due to excessive context size?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "Are there any ways to improve performance of large language models for telecom tasks?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "How do you deal with cases where the model gives correct answers but misses crucial details?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "How does finetuning with 1 epoch affect the overall accuracy score?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "What are some potential next steps for exploring RAG pipelines and fine-tuning LLMs for telecom applications?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "How does NDCG help in understanding retrieval system quality?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "What's the purpose behind designing a custom RAG pipeline for this study?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "How can we ensure our embedding model is well-trained with telecom-specific vocabulary and context?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "How does this improved info retrieval method deal with irrelevant texts?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "Can I use truncated embeddings throughout the entire retrieval process for better performance?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Is there a tutorial on creating custom loss functions like Ma-tryoshkaLoss?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Is it possible to add more models or stores to the pipeline for even better results?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "Can you tell me more about how document segments are retrieved based on user queries?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "Is there a study or research that shows the effectiveness of combining multiple steps in a pipeline?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "Can you tell me about the incremental fine-tuning technique?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "How does domain knowledge impact LLM performance for telecom-related queries?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "What other embedding models could potentially improve model performance for telecom tasks?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "Can you tell me about methods that improve model performance without increasing computing resources?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "What's the 'hallucination problem' in generative models and how's it addressed?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "What's the advantage of implementing advanced prompt engineering techniques for telecom tasks?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "How can I utilize the incremental fine-tuning technique in my project?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "What's the first step in the RAG pipeline?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Can I adapt this approach for my own non-telecom question pipeline?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Was there a notable decrease in performance with lower embedding dims?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "Can you describe a scenario where hybrid search is really helpful?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "How does TelecomGPT compare to other models like GPT-4, Llama-3, and Mistral?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "Are there any incremental approaches that can help with model finetuning and exhaustion?"
    },
    {
        "chunk": "in tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or\n\u201cFalcon\u201d [2] to answer the MCQs in the TeleQnA dataset.\nWe design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we",
        "question": "Can you provide more context on why optimizing LLMs is crucial for 5G-related technical questions?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "What's the best approach to load large datasets for NLP tasks?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "Did they encounter any scalability issues with their pipeline for large datasets?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "Can you tell me about document loading and segmenting in the 3GPP dataset?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "What features make a system efficient for complex query handling?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "How does the quantity of instructions impact model performance?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Is there a way to use supporting documents in this RAG pipeline?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "Can you explain why some models perform better than others when it comes to exhaustion during training?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Can you provide a quick summary of the evaluation accuracy for all approaches listed?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "Is there a way to enhance model performance without starting from scratch?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "What changed in our model config to get a 45% boost in accuracy?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "How does the quality of instructions affect the model's performance?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "How did they determine the optimal chunk size for their model?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "Is there a way to rapidly process input test data in MCQ format?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "What's the process of unsupervised fine-tuning a pre-trained Phi-2 model?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "How does fine-tuning contribute to the overall performance of the Phi-2 model?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "Are there specific steps to follow in segmenting a complex document?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "Is there any risk of getting irrelevant results when combining vector and BM25 retrieval?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "What's the outcome when we apply MRL method for optimizing embeddings?"
    },
    {
        "chunk": "use alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced\nresources, the amount of the dataset and the complexity of\nthe model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used",
        "question": "I'm working on a project and running into computation issues, any tips?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "How do you ensure that language models are optimized for efficient processing in telecom applications?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "How much of an improvement can you expect by combining multiple steps in a pipeline?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "Can you summarize the most effective techniques used in this study?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "How do you deal with irrelevant characters in text data?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "How does their methodology handle textual and tabular data?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "Can I use PEFT with low-rank adaptation for my telecom model?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "How is LangChain integrated with ChromaDB for efficient retrieval?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "What are some examples of code generation tasks that can be used with TelecomGPT?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Why did increasing the number of documents retrieved cause issues with Phi-2?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "What's the most common mistake people make when choosing their chunk size?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "How can we use the Hugging Face pipeline with the Phi-2 model for generating telecom-specific QA pairs?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "How can I improve retrieval efficiency for complex domain-specific queries?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "What's the purpose of skipping tables and images from documents?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "Can you tell me more about the Normalized Discounted Cumulative Gain metric?"
    },
    {
        "chunk": "information, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at\nmultiple dimensions, and a loss function is applied to both\nthe full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model",
        "question": "Can I get a breakdown of why they chose Ma-tryoshkaLoss over other custom loss functions?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "Can you recommend any recent papers or articles on LLMs in telecom?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "What's the most effective way to fine-tune an embedding model for telecommunications-related questions?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "How do people make their language models better at handling vocab and context?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "How does frontloading essential info into earlier dimensions affect overall accuracy?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "Can you point me to any recent papers on benchmarking LLMs in telecommunications?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "What's the maximum GPU memory available on NVIDIA RTX A5000 cards?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "Can you give an example of how the inference process works for a specific query?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "How does fine-tuning help with performance of large language models for telecom?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Is there a document or dataset related to 3GPP standards that I can use?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Can dimensionality reduction methods compromise retrieval system quality?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "How does the vocabulary in the segmented data help the embedding model?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "How can I improve the performance of my model through incremental fine-tuning?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "How does this system handle complex telecom queries?"
    },
    {
        "chunk": "(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality\nreduction without a substantial decrease in performance.\nWe used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the",
        "question": "Is NDCG useful for assessing retrieval systems across different dimension levels?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "Can you walk me through the process of using a combination of search techniques to boost overall performance?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "Can we use an incremental learning approach to address resource limits in telecom model training?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "How did you clean up the model's responses?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "How do I get an accuracy score for my model by directly using LLM-generated answers?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "What are some potential ways to improve model accuracy beyond just fine-tuning?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "How does large language model retrieval-augmented generation relate to telecom tasks?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "What's the role of augmentation in an RAG approach for NLP tasks?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "What are some ways to increase the accuracy of a language model with limited data?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "How does this process relate to the overall pipeline and its efficiency?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "Can you summarize the results of key experiments that demonstrated the effectiveness of various context retrieval approaches?"
    },
    {
        "chunk": "cause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search",
        "question": "How can I ensure that my model is performing optimally without running into exhaustion?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "Can you point me towards some recent surveys on LLM applications?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "Are there any specific research publications that contribute to TeleQnA?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "What's the iterative method you use to fix errors and improve accuracy?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "What's the relationship between fine-tuning models and the concept of transfer learning?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Can you explain why retaining crucial information is important in embeddings?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "How did the model struggle with telecom-specific instructions?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "How did you make sure the dataset was compatible with the model's architecture?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Can you explain why 100 words is chosen as the optimal chunk size?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "Are there any specific tools or libraries that can aid in the fine-tuning process?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "What are some common techniques used to fine-tune an embedding model for telecommunications applications?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "Can you provide some examples of how to use pre-processing in model fine-tuning?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "How does incremental fine-tuning compare to other methods in terms of accuracy improvements?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Why would someone use 100 words as a chunk size for document processing?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "What's the purpose of the TeleQnA dataset in relation to telecom and AI?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "Can you give me an example of a successful model fine-tuning experiment?"
    },
    {
        "chunk": "generated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as\noptions makes the task more practical and challenging.\nThe study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the",
        "question": "How does the RAG pipeline improve MCQs performance?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "What other alternatives can I explore if I'm facing similar computational limitations?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "Is there a specific method or library that's used to integrate document retrieval with embeddings?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "How do we ensure our embedding model is well-suited for telecom-specific use cases?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "How do I determine the best context size for MCQ question answering?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "How do I fine-tune an embedding model using datasets from multiple sources?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "Can you adapt a pre-trained model to a specific domain without retraining it completely?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "I'm trying to improve my model's accuracy, what worked for them?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "How long does it take to get results from this new hybrid method?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "Are there any particular benefits when using a combination of fine-tuned embeddings and the Phi-2 model?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "What's the benefit of designing a prompt for improved retrieval results?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "What are some common challenges when dealing with large documents?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What's involved in selecting the best model outputs?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "How was performance measured on the public leaderboard?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "What's the best way to optimize resource usage when dealing with huge training sets?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "Can you walk me through why it took twice as long for the new hybrid method?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "Can you explain how to improve the accuracy of an embedding model for telecom-related queries?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "Can you explain the impact of manual feedback on model performance in private leaderboards?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "What's the typical chunk size when working with large documents like yours?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "What's new in assessing LLMs specifically designed for telecommunications applications?"
    },
    {
        "chunk": "Telecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding\nPerformance through Large Language Model-based Text Enrich-\nment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic",
        "question": "What are some potential use cases for LLMs in 5G/6G networks?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "Are there any efficient ways to tune large models for telecom applications?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "How do researchers typically approach developing language models for telecom-related tasks?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "How is the QA pairs generated using 3GPP documents used in this approach?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "How do they generate synthetic QA pairs in their process?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "How do you keep track of answers and fix them manually?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "How does a manual feedback loop improve overall accuracy for models trained on private data?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "How does this approach impact our model's overall performance?"
    },
    {
        "chunk": "the full-size embeddings and the truncated ones. The loss\nvalues from each dimension are combined to create a final\nloss, which the model minimizes. The model was fine-tuned\nfor 25 epochs on the base model BAAI/bge-base-en-v1, and\nevaluated on the baseline score to quantify the improvements\nusing the same NDCG score metrics. The fine-tuned model\nsignificantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model",
        "question": "What's the relationship between model performance, storage efficiency, and retrieval speed in this context?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "Can you explain how to enhance LLM performance for specialized areas?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "What's the effect of having bad quality data on model performance?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Can you explain the concept of synthetic QA pair generation?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "What was the problem with using our initial servers for fine-tuning?"
    },
    {
        "chunk": "an existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a\nlarger chunk size for better context, the model fails to generate\ncorrect answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data",
        "question": "Why was skipping tables and images important for efficient data handling?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "What strategy did you use to handle big corpus and computational demands?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "How does this study relate to other work on improving model performance through incremental training?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "What's the best chunk size for a model like Phi-2?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "How does incremental fine-tuning of language models affect performance?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "What were some possible solutions to your computational limitations issues?"
    },
    {
        "chunk": "and the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-\n2 model for generating the answers as a baseline. For the\nsecond setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and",
        "question": "Were there any significant improvements in performance?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "Are there any benefits to using a three-component RAG pipeline in NLP?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "What's involved in fine-tuning a Phi-2 model for MCQs?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "Can you walk me through a process for generating synthetic QA pairs?"
    },
    {
        "chunk": "poor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.\nE. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved",
        "question": "How does restricting MCQ options impact the fine-tuned model's performance?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "Is there a significant improvement when using retrieval-augmented generation over other methods?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "How does our method ensure that both textual and tabular data are used effectively?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "Can you give me some practical tips for applying these findings in my own research?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "Can you suggest some techniques for regularizing and preventing overfitting in telecom-trained models?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "How does the human oversight impact the overall accuracy of the dataset?"
    },
    {
        "chunk": "The tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-\nters, allowing the model to operate more efficiently without\nsacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain",
        "question": "What's the best way to make large language models work efficiently without sacrificing performance?"
    },
    {
        "chunk": "incremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,\nthe conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main",
        "question": "What insights can be gained from studying the process of segmenting complex documents?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "What's the approximate number of hours required for one day of fine-tuning?"
    },
    {
        "chunk": "document retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was\nhighly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline",
        "question": "What's the key to rapidly identifying relevant document parts?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "What's the latest development in assessing large language models for telecom use cases?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "How does the iterative process improve over time, if at all?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "What's the constraint on using Phi-2 for generating answers to MCQs?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "Why did we choose not to include images in the embedding and fine-tuning phase?"
    },
    {
        "chunk": "model in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model",
        "question": "Are there any insights from these experiments regarding model fine-tuning strategies?"
    },
    {
        "chunk": "chunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-\ncause of the limitations exhaustion. For the model finetuning,TABLE I\nCOMPARED APPROACHES\nApproach Finetuned\nEmbeddingLLM\nModel\n(Phi-2)Epoch Chunk\nSizeManual\nFeedback\nLoop\n1. Baseline \u00d7 PT NA N/A \u00d7\n2. Ins. FT \u2713 Ins. FT 5 100 \u00d7\n3. FT Embed-\nding\nwith PT Phi-2\u2713 PT NA 100 \u00d7\n4. Inc. FT \u2713 Inc. FT 1 100 \u00d7\n5. Inc. FT \u2713 Inc. FT 1 500 \u00d7\n6. Inc. FT \u2713 Inc. FT 2 100 \u00d7\n7. Inc. FT \u2713 Inc. FT 2 500 \u00d7\n8. Inc. FT",
        "question": "Can you use smaller chunks to improve model efficiency without sacrificing accuracy?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "How does combining models like retrieval and generation enhance NLP accuracy?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "Are there any PEFT methods that don't require converting text, tables, and images into images?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "Is TeleQnA related to the study of large language models in telecommunication networks?"
    },
    {
        "chunk": "Along with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-\ntuning. We included the tables only at fine-tuning phase. Our\ntechnique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents",
        "question": "What was the main trade-off when deciding not to use image processing?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "What approaches help with embedding performances on SQuAD2 and NQ benchmarks?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "Can you give me more information on using large language models in 5G challenge?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Can you tell me about the Telecom Open QnA benchmark for evaluating telecom-specific LLMs?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "Are there any open-source large language models that demonstrate state-of-the-art performance?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "I've got a large document, what's the most efficient way to tokenize it?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "What's the best way to use a RAG system for answering telecom-specific MCQs?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "What are some key features of the TelecomGPT framework that set it apart from other models?"
    },
    {
        "chunk": "contains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse\nthe documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our",
        "question": "How do you make sure each text segment is processed quickly and efficiently?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "Are there any particular architectures or techniques that excel in handling telecom complexity?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "What are some potential issues with relying solely on a fine-tuned pre-trained embedding model?"
    },
    {
        "chunk": "Enhancing Large Language Models for Telecom\nNetworks Using Retrieval-Augmented Generation\nNasik Sami Khan, Md Mahibul Hasan, Md. Shamim Towhid, Saroj Basnet, Nashid Shahriar\nDepartment of Computer Science, University of Regina\n{nku618, mhr993, mty754, skb976, nashid.shahriar }@uregina.ca\nAbstract \u2014This paper presents a comprehensive approach for\nfine-tuning large language models (LLMs) for domain-specific\ntasks in the telecommunications field. We utilize a dataset with\n1,827 multiple-choice questions (MCQs) from 3GPP standard\ndocuments. A publicly available LLM named \u201cPhi-2\u201d is used to\nanswer the MCQs correctly. We develop a Retrieval-Augmented\nGeneration (RAG) pipeline to improve Phi-2 model\u2019s perfor-\nmance. The RAG pipeline comprises document segmentation,",
        "question": "Is the RAG pipeline adaptable to different large language models and datasets?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "What are some general best practices for managing computational challenges in model training?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "What kind of model is used for generating answers in this particular RAG implementation?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "What's the computational cost of generating QA pairs for 10,000 rows?"
    },
    {
        "chunk": "We used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.\nDue to the significant duration of the training, we decided to\nuse alternate methods to enhance the efficiency of the proce-\ndure. First, we tried with the paid Google Colab Pro platform\nfor the computation, but the session was timed out multiple\ntimes. Then finally, we ran our experiments on the Compute\nCanada server, which is equipped with an NVIDIA A100\nGPU featuring 40 GB of GPU memory. Despite the enhanced",
        "question": "Can using NVIDIA A100 GPU really make that big of a difference in performance?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Is there a specific table showing configura-\ntion settings with varying chunk sizes, fine-tuning techniques, and embedding methods?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Is there a technique for making language models more accurate?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "How do you fine-tune a model to answer telecom-specific multiple choice questions?"
    },
    {
        "chunk": "a limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs\nin most cases.\nWe also implemented a hybrid search technique that com-\nbines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant",
        "question": "How does BM25 help improve information retrieval in specialized sectors?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "I'm trying to train a model from scratch. Are there any general guidelines for handling complex domains like telecom?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Can you explain how they fine-tuned their embedding models?"
    },
    {
        "chunk": "E. Implementation of the RAG Pipeline\nIn this step, the fine-tuned Phi-2 model is used to generate\nanswers for multiple-choice questions within a RAG pipeline.\nThe inference process is designed to leverage the strengths\nof the custom fine-tuned embeddings and the unsupervised\nfine-tuned Phi-2 model, ensuring accurate and contextually\nrelevant responses. The initial step in the pipeline involved\ndocument retrieval and embedding integration. The segmented\ndocuments from step 1 in the pipeline were embedded using\nthe fine-tuned model, and these embeddings were stored in a\nvector database. We used the ChromaDB vector store, which\nis integrated with the LangChain library, to handle and retrieve\nthese embeddings. This ensured that the retrieval process was",
        "question": "Can you explain how the retrieval process gets improved when using the ChromaDB vector store?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "How can I balance performance and efficiency when fine-tuning a pre-trained model?"
    },
    {
        "chunk": "required for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments\nconducted as part of the ITU AI/ML in the 5G Challenge\n[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-",
        "question": "How does the TeleQnA dataset's characteristics impact the fine-tuning process?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "How does the RAG approach impact overall generative model performance and efficiency?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "I've heard of finetuning, but how does it work with embeddings?"
    },
    {
        "chunk": "in improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-\nstructions, leading to poor results. This outcome demonstrates\na limitation in the application of instruction-based fine-tuning\nwithin highly specialized domains. In all our experiments,\ngiven the input question we retrieved the top 1 matched\ndocument as the context from the vector database. Increasing\nthe number of documents retrieved led to the exhaustion of\nPhi-2\u2019s token limit, hence resulting in generating no outputs",
        "question": "Is instruction-based fine-tuning effective in highly specialized domains?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "Can you tell me about any new techniques in training ML models for niche industries?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "How do you improve the performance of question-answering models?"
    },
    {
        "chunk": "final dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-\ning unnecessary content, which resulted in a more stream-\nlined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these",
        "question": "What is considered essential information for this dataset cleanup?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "Can you give me examples of how to improve an embedding model for telecommunications-related questions?"
    },
    {
        "chunk": "Language Models Telecommunications Knowledge,\u201d Oct. 23, 2023,\narXiv: arXiv:2310.15051. Accessed: Aug. 16, 2024. [Online]. Available:\nhttp://arxiv.org/abs/2310.15051\n[13] Zindi, \u201cSpecializing Large Language Models for Telecom\nNetworks,\u201d Zindi. Accessed: Aug. 17, 2024. [Online]. Available:\nhttps://zindi.africa/competitions/specializing-large-language-models-for-\ntelecom-networks\n[14] P. Joshi, A. Gupta, P. Kumar, and M. Sisodia, \u201cRobust Multi Model\nRAG Pipeline For Documents Containing Text, Table & Images,\u201d\nin 2024 3rd International Conference on Applied Artificial Intel-\nligence and Computing (ICAAIC), Jun. 2024, pp. 993\u2013999. doi:\n10.1109/ICAAIC60222.2024.10574972",
        "question": "What are some best practices for training large language models in the context of telecom networks?"
    },
    {
        "chunk": "It included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and\nverified, they were assigned numeric values (1-5), which were\nrequired for the competition\u2019s submission format. The use of\nadvanced document retrieval, seamless embedding integration,\nand rigorous post-processing resulted in the creation of a\nhighly efficient RAG system for retrieving crucial information\nfrom large documents.\nIV. R ESULTS AND EVALUATION\nIn this section, we present the findings of our experiments",
        "question": "I'm wondering, what steps do you take to ensure the system's output is correct?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "What kind of embedding dimensions were evaluated in the baseline result?"
    },
    {
        "chunk": "8. Inc. FT\nwith HS\u2713 Inc. FT 2 100 \u00d7\n9. Inc. FT \u2713 Inc. FT 1 100 \u2713\nIns. = Instruction, Inc.= Incremental, PT = Pretrained, FT =\nFinetuning, HS = Hybrid Search\nwe implemented an incremental approach and experimented\nwith the model performance with 1 and 2 epochs. Finetuning\nwith 1 epoch was sufficient to provide good results in our\nexperiments. In approach (8), we applied a hybrid search\nmethod that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by",
        "question": "Can you explain how the performance of a model changes with more epochs?"
    },
    {
        "chunk": "in these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning\nIn this step, we focus on fine-tuning a pre-trained embed-\nding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with",
        "question": "How does the document cover telecom-specific vocabulary?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What processes involve removing non-essential info from model responses?"
    },
    {
        "chunk": "which resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding\nmodels, handling complex queries with sophisticated RAGpipeline frameworks, and use of advanced prompt engineer-\ning techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet",
        "question": "Can you tell me more about how this study improves LLM performance specifically in the telecom domain?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "How can I improve the performance of my language model for telecom-related tasks?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Is there a method to load documents directly from embeddings?"
    },
    {
        "chunk": "of the embedding model, and incremental fine-tuning of Phi-\n2. Our experiments show that accuracy greatly increased by\ncombining all the above-mentioned steps in the RAG pipeline.\nThe proposed approach outperforms the baseline by 45.20%\nin terms of accuracy. This study identifies the limitations of\ninstruction fine-tuning in specialized fields and explores the\npossibility of using sophisticated data processing with fine-tuned\nmodels to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-",
        "question": "What's the advantage of using sophisticated data processing with fine-tuned models?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "What model was used for generating answers to MCQs in our project?"
    },
    {
        "chunk": "[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.\n[6] A. Karapantelakis et al., \u201cUsing Large Language Models to Understand\nTelecom Standards,\u201d Apr. 12, 2024, arXiv: arXiv:2404.02929. doi:\n10.48550/arXiv.2404.02929.\n[7] C. Alberti, D. Andor, E. Pitler, J. Devlin, and M. Collins, \u201cSynthetic QA\nCorpora Generation with Roundtrip Consistency,\u201d Jun. 12, 2019, arXiv:\narXiv:1906.05416. doi: 10.48550/arXiv.1906.05416.\n[8] N. Harris, A. Butani, and S. Hashmy, \u201cEnhancing Embedding",
        "question": "Can you explain how to address inaccuracies in language models?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "Which AI model is less resource intensive compared to Falcon?"
    },
    {
        "chunk": "We used the Matryoshka Representation Learning (MRL)\ntechnique [11] to optimize embeddings across various dimen-\nsions. The technique is named after the famous Russian game\n\u201dMatryoshka dolls\u201d in which small dolls are nested within\nbigger ones. The concept brings a change in the understanding\nof data representation in the field of AI. This method allows the\nmodel to reduce the size of embeddings while retaining crucial\ninformation, thus ensuring both accuracy and efficiency.\nWe implemented a custom loss function, called Ma-\ntryoshkaLoss, that aggregates loss values across different\nembedding dimensions. It ensures that the model learns to\nfrontload essential information into the earlier dimensions of\nthe embedding vector. The model produces embeddings at",
        "question": "Is there a way to reduce the size of embeddings while keeping all the important info?"
    },
    {
        "chunk": "We design an RAG pipeline that utilizes the \u201cPhi-2\u201d model\nto generate the answers to the MCQs. The reason behind\nselecting \u201cPhi-2\u201d is that, it is less resource intensive compared\nto Falcon. Falcon has seven billion parameters whereas Phi-\n2 has two billion. The training and test sets are provided on\nTeleQnA dataset. One restriction on using \u201cPhi-2\u201d is that we\ncannot fine-tune the model using the options of the MCQs in\nthe training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic",
        "question": "We're generating answers to MCQs, what's the best model for this task?"
    },
    {
        "chunk": "technique ensures predominant behavior of both textual and\ntabular data and avoids complexity of image processing.\nIII. M ETHODOLOGY\nIn this section, we discuss our proposed approaches for\nanswering telecom-specific questions using the RAG pipeline\nin detail. We divided the main task into six sub-tasks for betterunderstanding. The phases are as follows: (1) Documents\nLoad and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It",
        "question": "Can you provide more details on the custom embedding model used?"
    },
    {
        "chunk": "Load and Segmentation, (2) Synthetic QA pair Generation,\n(3) Custom embedding model fine-tuning, (4) Fine-Tuning of\nthe Phi-2 Model, (5) Implementation of the RAG Pipeline,\nand (6) Answer extraction & post-processing step for result\nevaluation.\nA. Documents Loading and Segmenting\nIn the first step of the RAG pipeline, we load and segment\nthe raw documents from the 3GPP Release 18 dataset. It\ncontains technical standards related to the telecommunications\ndomain, and the 554 documents were provided in .docx format.\nWe segregate them into more manageable chunks to properly\nfit into the vector database.\nWe used the open-source Unstructured library to extract\nvarious text elements, such as narrative text, paragraphs, &\nlist items, from the source files. This library helped us parse",
        "question": "How do you manage large datasets like 3GPP's technical standards?"
    },
    {
        "chunk": "REFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).\n[4] Li, Jiarui, Ye Yuan, and Zehua Zhang. \u201dEnhancing llm factual accuracy\nwith rag to counter hallucinations: A case study on domain-specific\nqueries in private knowledge-bases.\u201d arXiv preprint arXiv:2403.10446\n(2024).\n[5] X. Lai et al., \u201cLISA: Reasoning Segmentation via Large Lan-\nguage Model,\u201d May 01, 2024, arXiv: arXiv:2308.00692. doi:\n10.48550/arXiv.2308.00692.",
        "question": "I'm looking for papers on LLMs, got any recommendations?"
    },
    {
        "chunk": "ding model with synthetically generated QA pairs produced\nin the earlier phase. The main goal is to maximize the\nperformance of the embedding model, especially by adapting\nvocabularies related to the telecommunication domain so that it\nmanages the domain-specific complexity and nuances robustly.\nWe divided the 10000 synthetically generated QA data with\na 90:10 ratio into training and testing sets to evaluate the\nmodel\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated",
        "question": "Is it possible to train a single model that handles multiple domains, including telecom?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "Are synthetic QA pairs used anywhere else besides SQuAD2 and NQ?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "How can I see if my model is overprocessing tokens and losing context?"
    },
    {
        "chunk": "and the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public\nleaderboard measured the performance of 50% of the test set,\nand the private leaderboard represented the full test set. In\nthe following sections, we discuss the experiment settings and\ntheir results.\nEvaluation Setting : Table I shows the different configura-\ntion settings we considered for our experiments with varying\nchunk sizes, fine-tuning techniques, and embedding methods.\nIn the first experiment, we considered the pre-trained phi-",
        "question": "Was there an experiment with pre-trained phi- models?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "Can you describe how the AI assistant processes input test data provided in JSON format?"
    },
    {
        "chunk": "the training set. A set of 3GPP specifications is shared with\nus that can be utilized as necessary. These documents contain\ninformation that is necessary to answer the MCQs correctly.\nThe TeleQnA dataset is created by collecting documents\nfrom 3GPP standards, research publications, and overview\n[12]. OpenAI\u2019s GPT-3.5 API is utilized to generate synthetic\nquestions from the collected and processed documents. The\ngenerated questions go through a human validation process\nto refine them. Therefore, the generated questions are validand, at the same time, challenging to answer. To answer the\nquestions, any model must have the domain knowledge. The\npresence of domain-specific acronyms in the questions and\nquestions with \u201cAll of the above\u201d or \u201cNone of the above\u201d as",
        "question": "What's the primary source of information for generating questions in TeleQnA?"
    },
    {
        "chunk": "In this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-\nsults. We prepared the dataset, ensuring compatibility with the\nmodel\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training",
        "question": "What techniques are used to clean and preprocess text data?"
    },
    {
        "chunk": "model\u2019s architecture. The text data from 554 source documents\nwas first cleaned by removing HTML tags, extra spaces, and\nother irrelevant characters. Tokenization is performed using\na sliding window technique, which is efficient when dealing\nwith larger documents. This approach maintained the inclusion\nof all important sections of the text during the training\nprocess, even if they surpassed the maximum token length.\nThe tokenizer was precisely configured to accommodate the\nspecifications of the Phi-2 model, establishing suitable token\nlengths and strides to enhance the process. We employed\na parameter-efficient fine-tuning method, particularly Low-\nRank Adaptation (LoRA). The model was initialized with\nquantization, which reduces the precision of model parame-",
        "question": "How does this Low-Rank Adaptation method change the way the model works?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "What techniques help stabilize the training process?"
    },
    {
        "chunk": "on various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document\ncategorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.",
        "question": "Can you tell me about the TelecomGPT framework and its benchmarks?"
    },
    {
        "chunk": "long document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2\nmodel. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s",
        "question": "What's the final step in this telecom-related query handling process?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "What's the advantage of using Matryoshka embeddings in retrieval systems?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "Can you give me some tips on choosing the right chunk size?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "Is there an optimal chunk size for most text processing tasks?"
    },
    {
        "chunk": "highly efficient and capable of rapidly identifying relevant\nparts of documents in response to a specific query.\nThe core of the inference process is the question-answering\npipeline. We processed the input test data, which was provided\nin a JSON structure. It contained question ID, question, op-\ntions, and category value in an MCQ-like pattern. The pipeline\nis configured to retrieve the most relevant document segmentsbased on the input question. These retrieved documents along\nwith the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering",
        "question": "What's the typical use case for applying this question-answering pipeline in real-world scenarios?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "Can you explain how to design a prompt template for generating synthetic QA pairs?"
    },
    {
        "chunk": "and contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did\nnot generate correct answers in most cases, hence resulting in\npoor performance. Instruction fine-tuning is highly sensitive to\nthe quality and quantity of the instruction and data provided.\nThe use of options of the MCQs for finetuning was restricted.\nThis resulted in a mismatch between the instructions and the\nactual output of the model and it is one major reason why the\nmodel could not generate the output properly.",
        "question": "What are some common pitfalls when using MCQs for training?"
    },
    {
        "chunk": "We used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists\nin these 10,000 data rows as they cover a large number of\ntelecom-specific vocabulary. Also, the synthetic QA generation\nprocess is computationally expensive and time-consuming togenerate for the whole dataset. The generated QA pairs were\nstored in a CSV file with each row containing an original text\nsegment and its corresponding generated questions.\nC. Embedding Model Fine-Tuning",
        "question": "Does the number of QA pairs affect the quality of the embedding model?"
    },
    {
        "chunk": "with a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better\nretrieval and generation accuracy. The use of MRL was pivotal\nin improving model performance. By distributing embedding\ninformation across multiple dimensions, this approach enabled\nthe pre-trained BAAI/bge-small-en-v1.5 model to efficiently\nretrieve relevant context and learn the domain-specific vocab-\nulary. The instruction fine-tuning did not perform well in our\nexperiments. The model struggled with telecom-specific in-",
        "question": "Can you explain why 100-token chunks were used in this experiment?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "How do you make sure data is handled efficiently before moving to the next steps in a pipeline?"
    },
    {
        "chunk": "significantly improved retrieval, especially at dealing with\ncomplex, domain-specific questions. It demonstrated the ad-\nvantages of Matryoshka embeddings in balancing performance\nwith storage efficiency. By utilizing truncated embeddings\nduring the initial retrieval phase, the system can quickly\nnarrow down relevant documents or contexts from a large\ncorpus.D. Fine-Tuning of the Phi-2 Model\nIn this phase, the focus was on fine-tuning the pre-trained\nPhi-2 model to enhance its performance, specifically for\nanswering telecom-related questions. The unsupervised fine-\ntuning process involved several sub-steps, including data\npreparation, tokenization, model initialization, and the applica-\ntion of advanced fine-tuning techniques to achieve optimal re-",
        "question": "How can I improve the performance of my retrieval system using advanced fine-tuning techniques?"
    },
    {
        "chunk": "correct answers during the testing phase. This step for loading\ndocuments and separating them into groups made sure that\nthe raw data was handled efficiently and prepared for the next\nsteps in our pipeline. In our data chunking, we skipped the\ntables and images from the documents.\nB. Synthetic QA Generation\nWe generate synthetic QA pairs with the segmented data\nfrom the previous step of our pipeline. These pairs are crucial\nfor fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM",
        "question": "What happens to documents with tables or images during chunking?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "How do we leverage the LangChain framework to enhance an embedding model's capabilities for telecom-specific queries?"
    },
    {
        "chunk": "bines vector-based and BM25 retrieval approaches to enhance\ninformation retrieval through semantic and lexical match-\ning. This improves coverage, decreases the risk of retrieving\nsemantically related but syntactically irrelevant texts, and\nprovides precise word matching. It is especially useful in\nspecialized sectors where contextual similarity and relevant\nterminology are both critical. The hybrid method addresses the\nconstraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.",
        "question": "Can you tell me about the benefits of this new retrieval approach?"
    },
    {
        "chunk": "\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.\nThe rest of the paper is organized as follows. Our literature\nsurvey is discussed in Section II. Section III provides a detailed\ndescription of our methodology. All the components of our\nproposed RAG pipeline are discussed in this section. The\nresults of our proposed approach compared with the selected\nbaseline are presented in Section IV. Continuing our work,",
        "question": "Can you provide some real-world examples of successful fine-tuning applications in NLP?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "What kind of evaluation was done before fine-tuning the model?"
    },
    {
        "chunk": "The baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s\nperformance in the field of telecommunications. Our best-\nperforming model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,",
        "question": "How did they achieve such significant improvements in accuracy?"
    },
    {
        "chunk": "[13]. Our primary focus is to fine-tune the Phi-2 model, fine-\ntune the embedding model, and implement a RAG pipeline\nto enhance the model\u2019s performance in answering telecom-\nspecific MCQs from the TeleQnA dataset [12]. The dataset\ncontains 1,827 MCQs, and is split into a training set and\ntest set with 1,461 and 366 questions, respectively. The com-\npetition also provided 554 supporting documents on 3GPP,\nand the technical standards related to the telecommunications\ndomain. We performed a series of experiments that involved\nvarious strategies for LLM & embedding model fine-tuning,\nand chunk size optimization to achieve the best accuracy\nscore for the competition. Each submission was evaluated on\nboth the public and private leaderboards, where the public",
        "question": "Can I get more info on what exactly TeleQnA dataset is?"
    },
    {
        "chunk": "The study leverages an RAG pipeline to enhance the Phi-\n2 model\u2019s accuracy in answering MCQs. The RAG pipeline\nis an approach to combining the strengths of the retrieval-\nbased model and the generation-based model to enhance the\noverall performance of any NLP task [3]. The retrieval model\nprovides context for the generative model. By utilizing the\ncontext, the generative model generates the correct output.\nThis RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG",
        "question": "Can you explain how the RAG approach enhances overall NLP performance?"
    },
    {
        "chunk": "the documents and relevant metadata, such as the 3GPP release\nnumber, which was extracted using regular expressions. Then\nthe documents were loaded and the text divided into smaller,\nmanageable chunks. Each chunk was 100 words in length, a\nsize chosen to ensure that the text segments were compact\nenough for efficient processing in subsequent stages of our\npipeline. For the document chunking, we appended the text to\nan existing segment or started a new one, depending on the\nlength of the current segment. We also experimented with a\n500-token chunk size with the assumption that more context\nwould result in better accuracy in extracting answers for MCQ\nquestions. However, our experiments revealed that the token\nlimit of the Phi-2 model is 2048 tokens. If we provide a",
        "question": "What's the token limit for the Phi-2 model and how does it affect chunking?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "What's the ideal context length for effective model finetuning?"
    },
    {
        "chunk": "the conclusion with our key findings and some future research\ndirections are discussed in Section V.II. R ELATED WORKS\nDocument loading and segmentation are two crucial pro-\ncesses for NLP tasks. Lai et al. introduced a system named\nLISA which can handle complex, implicit queries by segmen-\ntation documents based on user instructions. One of the main\ncapabilities of the tool is that it can produce segmentation\nfrom embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-",
        "question": "Is document loading essential for NLP tasks?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "Can you explain how to optimize overall performance in this context?"
    },
    {
        "chunk": "categorization, telecom code generation, and math modelling\nin telecommunications.\nZhou et al. surveyed LLMs in telecom and highlighted\nparameter-efficient fine-tuning (PEFT) methods including low-\nrank adaptation for fine-tuning big models [9]. The models can\nbe deployed to resource constraint telecom systems to improve\nefficiency and accuracy of configuration and troubleshooting.\nAlong with PEFT, we needed to follow an incremental learning\napproach to address resource limits in our training environ-\nment.\nOur RAG pipeline shares similarities with Josi et al.\u2019s one\n[14], particularly addressing multimodal data. Unlike their\nmethod of converting text, tables, and images into images,\nwe chose to skip the images in both the embedding and fine-",
        "question": "What approaches can be used to address resource limits in telecom model training?"
    },
    {
        "chunk": "the model still required a more efficient strategy in terms of\nresource usage. As a result, we adopted an incremental fine-\ntuning strategy.\nThis approach involved splitting the training dataset into\nthree subsets and incrementally fine-tuning the model on each\nsubset. Initially, the base Phi-2 model was fine-tuned on the\nfirst third of the dataset. This updated model was then used\nas the starting point for fine-tuning the next third of the\ndataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach",
        "question": "How do I ensure my model doesn't plateau during the incremental fine-tuning process?"
    },
    {
        "chunk": "performing model configuration reached a 67% accuracy on\nthe private leaderboard, improving the baseline score by\n45.20%. Significant improvements in accuracy are achieved\nby fine-tuning the pre-trained Phi-2 model and using MRL for\nembedding finetuning. The incremental fine-tuning technique\nproved efficient in managing the computational constraints,\nwhich resulted in a feasible solution for this task.\nFuture work could focus on including diverse document\nformats like summaries of tables, and image descriptions\nthrough a multi-modal RAG pipeline, which could enhance\nthe model\u2019s performance. Furthermore, instruction fine-tuning\nfor telecom-specific tasks, exploring other larger embedding",
        "question": "What was the impact of including MRL for embedding finetuning?"
    },
    {
        "chunk": "method that combines both vector and keyword-based search\nmechanisms for context retrieval. The difference between ap-\nproaches (4) and (9) is that, in the first experiment, the answers\ngenerated by LLM were directly used to get the accuracy\nscore. Whereas, in the last experiment, we applied a manual\nfeedback loop to rectify the few incorrect labels generated by\nLLM. It significantly improved the overall accuracy of the\nmodel in our experiments.\nEvaluation Results and Discussion : Table II summarizes\nthe results of our key experiments, highlighting the combina-\ntion of techniques used, and their corresponding performance\non the public and private leaderboards.\nTABLE II\nEVALUATION ACCURACY OF ALL THE APPROACHES\nApproach Public\nLeaderboard\nAccuracyPrivate\nLeaderboard\nAccuracy",
        "question": "How do I implement a manual feedback loop in my own model to rectify incorrect labels generated by LLMs?"
    },
    {
        "chunk": "the accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic\ndatasets significantly improves the performance of QA models\non benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances",
        "question": "How can LLMs be used to address vocabulary and context limitations in QA models?"
    },
    {
        "chunk": "This RAG approach also helps the generative model to address\nthe well-known hallucination problem [4]. Because of all these\nadvantages of the RAG approach, we design an RAG pipeline\nto solve this challenge. Any RAG pipeline can be divided into\nthree components: retrieval, augmentation, and generation. We\ncontribute to each of these components in our proposed RAG\npipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the",
        "question": "How does the augmentation component contribute to the overall RAG pipeline?"
    },
    {
        "chunk": "second setting, we explored the instruction finetuned phi-2\nmodel with a finetuned BAAI/bge-small-en-v1.5 embedding\nmodel. As the performance improvement was not significant,\nwe tried the custom embedding model with a pre-trained phi-\n2 model. For all the other compared approaches (4\u20139), we\nused the custom embedding model with an unsupervised and\nincremental finetuned phi-2 model with different document\nchunk sizes and training epochs.\nWe used two different chunk sizes, respectively 100 and\n500 tokens, to provide a balanced context retrieval while\nconsidering the token constraints of the Phi-2 model. The\n100-token size provided a suitable amount of context without\nexhausting the model limit, whereas with the 500-token level\nin many cases, the model could not generate any answers be-",
        "question": "What's the best way to tweak model parameters for improved results?"
    },
    {
        "chunk": "models to improve performance even more.\nIndex Terms \u2014retrieval-augmented generation, fine-tuning, em-\nbeddings, large language models, Telecom, LoRA\nI. I NTRODUCTION\nLarge language models\u2019 (LLMs) rapid evolution has rev-\nolutionized natural language processing (NLP) in numerous\ndomains. However, the use of LLMs in the telecommunica-\ntions sector has not been extensively implemented, especially\nin tasks that require specific domain knowledge, such as\nproviding answers to technical questions based on 3GPP\nstandards. Using the TeleQnA [12] dataset, the ITU AI/ML\nin 5G Challenge brings an opportunity to address this gap by\nemphasizing on optimizing LLMs for telecom-specific tasks.\nIn this challenge, the task is to utilize either \u201cPhi-2\u201d [1] or",
        "question": "How does ITU AI/ML in 5G Challenge help optimize LLMs for telecom?"
    },
    {
        "chunk": "sacrificing performance. LoRA is a technique that allows for\nfine-tuning with a smaller set of parameters, resulting in a sub-\nstantial reduction in computing expenses while maintaining or\nimproving the model\u2019s performance. This technique modifies\nonly a subset of the model\u2019s parameters, allowing the model\nto adapt to the specific requirements of the telecom domain\nwithout the need for extensive retraining of the entire model.\nWe used gradient checkpointing and warmup ratios, which are\ntechniques that help stabilize the training process.\nGiven the computational limitations of our initial servers\nequipped with NVIDIA RTX A5000 and NVIDIA RTX 3090\nGPUs, both having 24 GB of GPU memory, we faced signifi-\ncant delays during the fine-tuning process on the full dataset.",
        "question": "What methods help reduce computational expenses during fine-tuning?"
    },
    {
        "chunk": "with the questions were then passed to the fine-tuned Phi-\n2 model to generate an answer. A custom prompt template\ninstructed the model to select the correct answer from the\nprovided multiple-choice options. The prompt is stated below:\nInstruction: You are an AI assistant for answering\nmultiple choice questions from the provided context.\nYou are given the following extracted parts of a\nlong document and a question with some options\nnumbered with capital English letters. Just select the\ncapital English letter of the option that answers the\nquestion correctly. No need to explain further.\nThis pipeline was effective in handling complex telecom-\nrelated queries, as it combined the robust retrieval capabilities\nof the vector store with the generative abilities of the Phi-2",
        "question": "How do I get my own system to handle complex telecom queries like that?"
    },
    {
        "chunk": "model. The generated answers are then processed in the next\nstep of the pipeline.\nF . Post-Processing and Manual Feedback Loop\nThe final phase of the pipeline involved post-processing\nthe previous phase\u2019s generated answers to improve their cor-\nrectness and ensure they adhered to the specific format for\nresult submission. This step is crucial for selecting the model\u2019s\noutputs, optimizing overall performance, and preparing the\nfinal dataset for submission. Initially, the fine-tuned Phi-2\nmodel\u2019s responses were retrieved and cleaned using regular\nexpressions to rigorously refine the answers, while ensuring\nthat only essential information, especially the single letter\ncorresponding to the multiple-choice alternatives (A/B/C/D/E),\nwas preserved. The processes included systematically remov-",
        "question": "What tools are used to refine and clean model outputs?"
    },
    {
        "chunk": "ment and Rewriting,\u201d Apr. 18, 2024, arXiv: arXiv:2404.12283. doi:\n10.48550/arXiv.2404.12283.\n[9] H. Zhou et al., \u201cLarge Language Model (LLM) for Telecommu-\nnications: A Comprehensive Survey on Principles, Key Techniques,\nand Opportunities,\u201d May 17, 2024, arXiv: arXiv:2405.10825. doi:\n10.48550/arXiv.2405.10825.\n[10] H. Zou et al., \u201cTelecomGPT: A Framework to Build Telecom-Specfic\nLarge Language Models,\u201d Jul. 12, 2024, arXiv: arXiv:2407.09424. doi:\n10.48550/arXiv.2407.09424.\n[11] \u201cIntroduction to Matryoshka Embedding Models.\u201d Accessed: Aug. 12,\n2024. [Online]. Available: https://huggingface.co/blog/matryoshka\n[12] A. Maatouk, F. Ayed, N. Piovesan, A. De Domenico, M. Debbah,\nand Z.-Q. Luo, \u201cTeleQnA: A Benchmark Dataset to Assess Large",
        "question": "Is there a recent study on developing and benchmarking LLMs specifically designed for telecommunications?"
    },
    {
        "chunk": "model\u2019s performance both during and after the fine-tuning pro-\ncess. We used Hugging Face datasets and sentence-transformer\nlibraries for this task. Before the fine-tuning process, we\ncreated a baseline result using a pre-trained model, BAAI/bge-\nbase-en-v1.5. This baseline served as a reference point to mea-\nsure the effectiveness of our fine-tuning results. We evaluated\nthe model using the Normalized Discounted Cumulative Gain\n(NDCG) metric, which is useful in assessing the quality of\nretrieval systems. The baseline model was evaluated across\nmultiple embedding dimensions (768, 512, 256, 128, and 64)\nto provide a comprehensive understanding of its performance\nat different levels of embedding truncation. This step was\nvital in assessing the model\u2019s ability to execute dimensionality",
        "question": "What kind of understanding did they gain from evaluating their model across multiple embedding dimensions?"
    },
    {
        "chunk": "ing techniques could be explored. This study improves the\nperformance of LLM and implements the RAG pipeline for\ntelecom domain solutions. Our proposed methods have the\npotential to be applied in other specialized areas such as\ncybersecurity, healthcare, law, or finance, where they can\nenhance general-purpose LLMs by fine-tuning them to meet\nthe unique demands of each field.\nREFERENCES\n[1] Javaheripi, M. and Bubeck, S. (2023) \u201cPhi-2: The surprising power\nof small language models, Microsoft Research.\u201d (Accessed: 20 August\n2024).\n[2] E. Almazrouei et al., \u201cFalcon-40B: an open large language model with\nstate-of-the-art performance\u201d, 2023.\n[3] Gao, Yunfan, et al. \u201cRetrieval-augmented generation for large language\nmodels: A survey.\u201d arXiv preprint arXiv:2312.10997 (2023).",
        "question": "What new techniques can be applied to improve LLM performance?"
    },
    {
        "chunk": "dataset. Finally, the process was repeated for the last subset.\nThis stepwise fine-tuning allowed us to manage the large\ncorpus and computational demands effectively. Each phase\nof fine-tuning on 33% of the dataset took approximatelyFig. 1. An overview of the proposed RAG pipeline\none day to complete. This incremental fine-tuning approach\nprovided a practical solution to the computational challenges\nand contributed to the overall efficiency of the fine-tuning\nprocess. We ran our model for 3 epochs, but our experiment\nshowed that only 1 epoch of training was sufficient to get\nthe best result in the competition\u2019s evaluation phase, which\nwe will discuss in the result and evaluation section. We also\nimplemented instruction fine-tuning on the dataset, but it did",
        "question": "How did you determine that 1 epoch was sufficient for your experiment?"
    },
    {
        "chunk": "on benchmarks like SQuAD2 and Natural Questions (NQ). [7]\nHarris et al. also followed a similar approach of generating\nsynthetic QA pairs to improve the performance of the embed-\nding model. To address the limitation of vocabulary and lack\nof context, authors use LLMs to rewrite input texts which\nshowed significant improvement in embedding performances\non various datasets for embedding model\u2019s fine tune. [8]\nZou et al. proposed TelecomGPT, a telecom-specific LLM\nframework [10]. Authors gathered and prepared pre-training,\ninstruction, and alignment datasets as well as created Telecom\nMath Modelling, Telecom Open QnA, and Telecom Code\nbenchmarks for evaluation. TelecomGPT surpassed GPT-4,\nLlama-3, and Mistral in these benchmarks for 3GPP document",
        "question": "What's a good approach to take when working with vocabulary and context in embeddings?"
    },
    {
        "chunk": "constraints of vector-based search alone, resulting in a more\nextensive and accurate retrieval procedure. However, in our\nexperiments, the inference time was twice as long as that of the\nvector search. This is because two different methods were used\nsimultaneously, resulting in a time-inefficient pipeline given\nthe deadline constraint of the competition.\nThe baseline results using the pre-trained Phi-2 with the pre-\ntrained BAAI/bge-small-en-v1.5 model served as a benchmark\nfor our experiments. The significant difference between our\nbest result and baseline demonstrates the efficiency of our\npipeline in greatly enhancing the performance of the model.\nV. C ONCLUSION & F UTURE WORKS\nThe goal of this study is to improve the Phi-2 model\u2019s",
        "question": "How does the efficiency of a search pipeline affect performance?"
    },
    {
        "chunk": "pipeline. Our main contributions are discussed below.\n\u2022We generate QA pairs using the segmented chunks from\n3GPP documents and fine-tune the pre-trained embedding\nmodel on the generated QA pairs to improve the retrieval\nprocess. With this fine-tuning, the embedding model\ncan retrieve related context by which the MCQ can be\nanswered.\n\u2022A prompt is carefully designed considering how the\n\u201cPhi-2\u201d model was originally trained. We augment the\nprompt with the retrieved chunked documents during the\ninference.\n\u2022To improve the generation process, we fine-tune the \u201cPhi-\n2\u201d model incrementally on the shared 3GPP documents.\nThis fine-tuned model performs better than the originally\ntrained \u201cPhi-2\u201d which indicates the effectiveness of our\nincremental fine-tuning process.",
        "question": "What are some general guidelines for designing effective prompts?"
    },
    {
        "chunk": "lined and unified data format. Despite the automated cleaning\nprocess, just a small fraction of answers (0.65% to 0.85%)\nhad issues that required manual intervention. For example, the\nmodel gave the right responses, but the option number wasnot indicated in the generated text. Only one to five questions\nwere left unanswered by the model. To deal with these\noutlier cases, the pipeline includes a manual feedback loop.\nIt included evaluating the results, identifying any remaining\nerrors, and manually fixing them to ensure that each answer\nfollowed the expected structure. This iterative method was\ncritical for maintaining high accuracy in the final dataset,\nespecially in situations when the model\u2019s output differed from\nthe correct answer. After the answers had been cleaned and",
        "question": "Is the manual feedback loop part of a larger quality control process?"
    },
    {
        "chunk": "from embedding directly. This system demonstrates its zero-\nshot abilities and robust performances even with limited data\nfor fine-tuning. [5]\nKarapantelakis et al. explored the use of LLM for under-\nstanding telecommunication standards. They fine-tuned LLMs\nto handle large and complex documents by providing faster\naccess to relevant information. They also demonstrate how pre-\nprocessing as well as segmentation can contribute to increasing\nthe accuracy of a fine-tuned model. [6]\nTo improve performance of question-answer (QA) models,\nAlberti et al. developed a technique to generate synthetic QA\npairs. The overall process involves generating questions based\non segmented text and validating through answer consistency\nchecks. The authors demonstrate how utilizing these synthetic",
        "question": "Are there any strategies for making language models more robust?"
    },
    {
        "chunk": "for fine-tuning the embedding model and for enhancing its\nability to accurately process the telecom-specific questions.\nEach segment from the previous chunks is provided as the\ncontext for generating relevant questions. To generate the\nQA pairs, we designed a prompt template to ensure that\neach document chunk is provided as an input and the LLM\ngenerates a synthetic question from that document chunk.\nWe used the pre-trained Phi-2 model from the Hugging Face\npipeline and LangChain framework for this task. We generated\na total of 10,000 synthetic QA pairs from the segmented\ndata, instead of creating QA pairs for the whole dataset. Our\nintuition is that, in the next step of our pipeline, the embedding\nmodel will be well-trained with the vocabulary that exists",
        "question": "How do we train an embedding model to handle telecom-specific vocabulary and context?"
    },
    {
        "chunk": "AccuracyPrivate\nLeaderboard\nAccuracy\n1. Baseline 0.2158 0.218\n2. Ins. FT 0.3743 0.409\n3. FT Embedding with\nPT Phi-20.4645 0.524\n4. Inc. FT 0.5519 0.603\n5. Inc. FT 0.5355 0.561\n6. Inc. FT 0.3798 0.384\n7. Inc. FT 0.5301 0.586\n8. Inc. FT with HS 0.5846 0.6595\n9. Inc. FT 0.6092 0.670\nFrom Table II, it can be seen that our best-performing\napproach involved incremental fine-tuning of the Phi-2 model\nwith a 100-token chunk size, which achieved a 67% privateleaderboard accuracy, substantially improving the baseline\naccuracy of 21.8%. This configuration allowed the model to\nbetter adapt to the dataset\u2019s pattern. The 100-token chunk\nsize was ideal for keeping crucial context without exceeding\nthe model\u2019s token processing capabilities, resulting in better",
        "question": "What configuration led to the highest accuracy in Table II?"
    }
]